# 场景 3：流式数据清洗

## 场景概述

**业务需求**：实时清洗Kafka数据流，写入数据湖

**执行模式**：Streaming（流式处理）

**运行方式**：7x24 小时持续运行

**吞吐量**：10000 条/秒

**延迟要求**：< 10 秒

**核心特点**：持续运行、有状态、窗口聚合、实时写入

---

## 数据流程图

```text
Kafka 原始数据流
  ↓
Schema 验证和格式化
  ↓
数据清洗和过滤
  ↓
窗口聚合（每 5 分钟）
  ↓
写入数据湖（Parquet）
  ↓
更新 Kafka Offset
```

---

## 详细步骤

### 1. 消费 Kafka 数据流

**输入**：Kafka Topic `raw_events`

**消息格式**：
```json
{
  "event_id": "EVT_123",
  "user_id": "USER_456",
  "event_type": "click",
  "timestamp": "2025-12-04T10:30:00Z",
  "properties": {...}
}
```

**处理模式**：持续消费

**吞吐量**：10000 消息/秒

### 2. Schema 验证和格式化

**输入**：原始事件流

**处理**：
- 验证必填字段（event_id, user_id, timestamp）
- 类型转换（timestamp 字符串 → Unix 时间戳）
- 字段标准化（统一命名规范）
- 去除非法字符

**输出**：验证后的事件流

**延迟**：< 1 秒

**容错**：验证失败的消息写入死信队列

### 3. 数据清洗和过滤

**输入**：验证后的事件流

**处理**：
- 去重（基于 event_id，5 分钟窗口）
- 过滤测试用户
- 过滤异常数据（timestamp 超出合理范围）
- 字段补全（缺失字段填充默认值）

**输出**：清洗后的事件流

**延迟**：< 2 秒

**状态管理**：维护 5 分钟滑动窗口的 event_id 集合

### 4. 窗口聚合

**输入**：清洗后的事件流

**窗口设置**：5 分钟滚动窗口

**处理**：
- 按窗口聚合统计（事件数、用户数）
- 生成窗口元数据（窗口起止时间）

**输出**：聚合后的批次数据

**延迟**：窗口结束后 5 秒触发

### 5. 写入数据湖

**输入**：窗口聚合的批次数据

**处理**：
- 按小时分区写入（`hour={{window_hour}}`）
- Parquet 格式压缩
- 原子性写入（先写临时文件，再 rename）

**输出**：数据湖分区文件

**数据量**：每 5 分钟约 300MB（3百万条）

**延迟**：< 5 秒

### 6. Offset 管理

**处理**：
- 窗口数据写入成功后提交 Offset
- 支持断点续传

**Checkpoint 间隔**：每 5 分钟

---

## 特点分析

### ✅ 验证的设计要点

1. **持续运行**：无明确完成状态，7x24 运行
2. **有状态**：维护去重窗口状态
3. **窗口聚合**：按时间窗口批量写入
4. **Checkpoint**：支持故障恢复
5. **服务依赖**：依赖 Kafka 可用性

### 核心依赖关系

| 组件 | 依赖类型 | 等待条件 | 说明 |
|------|---------|---------|------|
| Schema 验证 | **服务健康** | Kafka 可用 | 持续消费 |
| 数据清洗 | **running** | 上游流任务运行中 | 流式依赖 |
| 窗口聚合 | **running** | 上游流任务运行中 | 流式依赖 |
| 写入数据湖 | **服务健康** | 数据湖可用 | 持续写入 |

---

## Pipeline 定义

```yaml
pipeline:
  name: "streaming_data_cleaning"
  description: "流式数据清洗流水线"
  
  inputVariables:
    - name: "kafka_bootstrap_servers"
      type: "string"
      description: "Kafka 集群地址"
    - name: "input_topic"
      type: "string"
      description: "输入 Topic"
    - name: "output_path"
      type: "string"
      description: "数据湖输出路径"
  
  nodes:
    # Kafka 源（模拟为独立节点，实际可能是外部服务）
    - alias: "kafka_source"
      type: "task"
      executionMode: "streaming"
      trigger: "cron:@startup"
      taskRef:
        namespace: "com.company.streaming"
        name: "kafka_connector"
        version: "1.0.0"
      inputs:
        bootstrap_servers: "{{pipe.kafka_bootstrap_servers}}"
        topic: "{{pipe.input_topic}}"
      healthCheck:
        enabled: true
        interval: 30
        endpoint: "http://kafka:9092/health"
    
    # 流式清洗任务
    - alias: "stream_cleaner"
      type: "task"
      executionMode: "streaming"
      trigger: "event:kafka_source.statusChanged"
      skipWhen: "kafka_source.health != 'healthy'"
      stopWhen: "kafka_source.health == 'unhealthy'"
      alertWhen: "stream_cleaner.metrics.lag > 100000"
      taskRef:
        namespace: "com.company.streaming"
        name: "flink_data_cleaner"
        version: "1.0.0"
      inputs:
        # Kafka 配置
        kafka_bootstrap_servers: "{{pipe.kafka_bootstrap_servers}}"
        input_topic: "{{pipe.input_topic}}"
        consumer_group: "data_cleaner_group"
        
        # 处理配置
        dedup_window: "5m"
        aggregation_window: "5m"
        checkpoint_interval: "5m"
        
        # 输出配置
        output_path: "{{pipe.output_path}}"
        output_format: "parquet"
        partition_by: "hour"
        
        # Schema 配置
        required_fields: ["event_id", "user_id", "timestamp"]
        dead_letter_topic: "raw_events_dlq"
      
      alertConfig:
        channel: "slack"
        severity: "critical"
        message: "Kafka 消费延迟过高: {{stream_cleaner.metrics.lag}}"
      
      executionPolicy:
        resourceId: "flink_cluster"
```

---

## 关键设计要点

### 1. 持续运行

```yaml
executionMode: "streaming"
restartPolicy: "on-failure"
```

任务失败自动重启，保证服务可用

### 2. 状态管理

```yaml
dedup_window: "5m"
checkpoint_interval: "5m"
```

维护窗口状态，支持故障恢复

### 3. 窗口聚合

```yaml
aggregation_window: "5m"
```

按窗口批量写入，平衡延迟和吞吐

### 4. 容错机制

```yaml
dead_letter_topic: "raw_events_dlq"
```

验证失败的消息不丢失

---

## 运行时行为

### 正常运行

```text
持续消费 Kafka 消息
  ↓
每秒处理 10000 条
  ↓
维护 5 分钟去重窗口
  ↓
每 5 分钟触发一次写入
  ↓
写入数据湖（300MB）
  ↓
提交 Kafka Offset
  ↓
循环...
```

### 故障恢复

```text
任务因异常终止
  ↓
自动重启（restartPolicy: on-failure）
  ↓
从上次 Checkpoint 恢复状态
  ↓
从上次提交的 Offset 继续消费
  ↓
窗口内的数据可能重复处理（需幂等性）
```

### 延迟监控

```text
正常情况：
  消费延迟 < 10 秒
  Kafka Lag < 10000

告警情况：
  消费延迟 > 60 秒 → 告警
  Kafka Lag > 100000 → 告警
```

---

## 监控指标

| 指标 | 阈值 | 说明 |
|------|------|------|
| 服务可用性 | > 99.9% | 自动重启保障 |
| 吞吐量 | > 8000 条/秒 | 实际处理能力 |
| 端到端延迟 P99 | < 30 秒 | 消费到写入的延迟 |
| Kafka Lag | < 50000 | 消费延迟 |
| 错误率 | < 5% | Schema 验证失败率 |
| Checkpoint 成功率 | > 99% | 状态保存成功率 |

---

## 与场景 1、2 对比

| 维度 | 场景 1（批量） | 场景 2（增量） | 场景 3（流式） |
|------|--------------|--------------|--------------|
| 执行模式 | Batch | Scheduled Batch | **Streaming** |
| 触发方式 | 手动 | 定时（cron） | **消息驱动** |
| 完成状态 | 有 | 有 | **无** |
| 延迟 | 小时级 | 小时级 | **秒级** |
| 状态管理 | 无 | 无 | **有（窗口）** |
| 容错方式 | 重跑 | 重跑/补跑 | **Checkpoint** |

---

## 总结

这是一个**典型的 Streaming 数据处理场景**，验证了：

1. **streaming 执行模式**：持续运行，无完成状态
2. **有状态处理**：窗口去重
3. **服务依赖**：依赖 Kafka 健康
4. **Checkpoint 机制**：故障恢复
5. **实时性**：秒级延迟
