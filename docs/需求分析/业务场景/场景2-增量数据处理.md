# 场景 2：增量数据处理

## 场景概述

**业务需求**：每天凌晨处理前一天的增量数据

**执行模式**：Scheduled Batch（定时批处理）

**调度频率**：每天凌晨 2:00（cron: `0 2 * * *`）

**数据规模**：每日 30GB 增量

**执行时长**：1 小时

**核心特点**：定时调度、时间分区、增量处理、补跑支持

---

## 数据流程图

```text
增量数据提取（T-1 日）
  ↓
数据去重和清洗
  ↓
按业务维度聚合
  ↓
增量写入目标表（分区）
  ↓
数据验证
```

---

## 详细步骤

### 1. 增量数据提取

**触发时间**：每天凌晨 2:00

**时间分区**：`execution_date = T-1`（前一天）

**输入**：
- 用户行为日志（按日期分区）
- 交易订单数据（按日期分区）

**处理**：
- 按 `ds={{execution_date}}` 读取分区
- 数据格式标准化
- 过滤测试数据

**输出**：增量原始数据

**数据量**：约 30GB（每日 1 亿条记录）

**执行时间**：10 分钟

### 2. 数据去重和清洗

**输入**：增量原始数据

**处理**：
- 基于主键去重（同一天内的重复）
- 跨天去重（避免与历史数据重复）
- 异常值过滤
- 字段补全

**输出**：清洗后的增量数据

**数据量**：约 28GB

**执行时间**：15 分钟

### 3. 按业务维度聚合

**输入**：清洗后的增量数据

**处理**：
- 按用户 ID 聚合（用户每日统计）
- 按商品 ID 聚合（商品每日统计）
- 计算增量指标（浏览量、点击率、转化率等）

**输出**：
- 用户每日汇总表
- 商品每日汇总表

**数据量**：约 5GB

**执行时间**：20 分钟

### 4. 增量写入目标表

**输入**：聚合后的数据

**处理**：
- 写入用户汇总表（分区 `ds={{execution_date}}`）
- 写入商品汇总表（分区 `ds={{execution_date}}`）
- 使用 INSERT OVERWRITE 确保幂等性

**输出**：分区数据

**执行时间**：10 分钟

### 5. 数据验证

**输入**：写入的分区数据

**处理**：
- 检查分区是否存在
- 验证数据量（与历史均值对比）
- 检查关键指标（用户数、订单数）

**输出**：验证报告

**执行时间**：5 分钟

---

## 特点分析

### ✅ 验证的设计要点

1. **定时调度（cron）**：每天固定时间自动触发
2. **时间分区（execution_date）**：处理特定日期的数据
3. **增量处理**：只处理新增数据，不重复计算历史
4. **幂等性**：重跑同一天的数据结果一致
5. **补跑（catchup）**：历史缺失日期自动补跑

### 核心依赖关系

| 步骤 | 依赖类型 | 等待条件 | 说明 |
|------|---------|---------|------|
| 数据去重 | **data_ready** | 提取完成且分区存在 | 验证分区数据 |
| 业务聚合 | **completed** | 去重完成 | 串行依赖 |
| 增量写入 | **completed** | 聚合完成 | 串行依赖 |
| 数据验证 | **data_ready** | 分区写入完成 | 验证数据质量 |

---

## Pipeline 定义

```yaml
pipeline:
  name: "incremental_data_processing"
  description: "增量数据处理流水线"

  trigger:
    cron: "0 2 * * *"  # 每天凌晨 2:00
    timezone: "Asia/Shanghai"
    catchup: true       # 支持补跑历史

  inputVariables:
    - name: "execution_date"
      type: "date"
      description: "执行日期（自动传入）"

  nodes:
    # 步骤 1：数据提取
    - alias: "extract_increment"
      type: "task"
      executionMode: "batch"
      trigger: "cron:0 2 * * *"
      retryWhen: "!partition_exists(source_table, execution_date)"
      failWhen: "retry_count >= 3"
      taskRef:
        namespace: "com.company.etl"
        name: "incremental_extractor"
        version: "1.0.0"
      inputs:
        source_tables:
          - "user_behavior_logs"
          - "transaction_orders"
        partition_key: "ds"
        partition_value: "{{execution_date}}"
      retryPolicy:
        maxAttempts: 3
        interval: 300
        backoff: "fixed"
      executionPolicy:
        timeout: 600

    # 步骤 2：去重清洗
    - alias: "dedup_clean"
      type: "task"
      executionMode: "batch"
      trigger: "event:extract_increment.succeeded"
      taskRef:
        namespace: "com.company.etl"
        name: "deduplicator"
        version: "1.0.0"
      inputs:
        input_path: "{{extract_increment.output_path}}"
        dedup_keys: ["user_id", "timestamp"]
      executionPolicy:
        timeout: 900

    # 步骤 3：业务聚合
    - alias: "aggregate"
      type: "task"
      executionMode: "batch"
      trigger: "event:dedup_clean.succeeded"
      taskRef:
        namespace: "com.company.etl"
        name: "aggregator"
        version: "1.0.0"
      inputs:
        input_path: "{{dedup_clean.output_path}}"
        group_by_dimensions:
          - "user_id"
          - "item_id"
        metrics:
          - "pv_count"
          - "click_count"
          - "order_count"
      executionPolicy:
        timeout: 1200

    # 步骤 4：增量写入
    - alias: "write_increment"
      type: "task"
      executionMode: "batch"
      trigger: "event:aggregate.succeeded"
      skipWhen: "aggregate.output.row_count == 0"
      taskRef:
        namespace: "com.company.etl"
        name: "partition_writer"
        version: "1.0.0"
      inputs:
        input_path: "{{aggregate.output_path}}"
        target_tables:
          - name: "dw.user_daily_summary"
            partition: "ds={{execution_date}}"
          - name: "dw.item_daily_summary"
            partition: "ds={{execution_date}}"
        write_mode: "overwrite"  # 幂等性
      executionPolicy:
        timeout: 600

    # 步骤 5：数据验证
    - alias: "validate"
      type: "task"
      executionMode: "batch"
      trigger: "event:write_increment.succeeded"
      failWhen: "validate.output.row_count < 1000000"
      alertWhen: "validate.output.null_ratio > 0.01"
      taskRef:
        namespace: "com.company.etl"
        name: "data_validator"
        version: "1.0.0"
      inputs:
        tables_to_check:
          - "dw.user_daily_summary"
          - "dw.item_daily_summary"
        partition: "ds={{execution_date}}"
        validation_rules:
          - check: "row_count"
            min: 1000000
          - check: "null_ratio"
            column: "user_id"
            max: 0.01
      alertConfig:
        channel: "email"
        severity: "warning"
        message: "数据质量告警: 空值率 {{validate.output.null_ratio}}"
      executionPolicy:
        timeout: 300
```

---

## 关键设计要点

### 1. 定时调度

```yaml
trigger:
  cron: "0 2 * * *"
  timezone: "Asia/Shanghai"
```

每天自动执行，无需手动触发

### 2. 时间分区

```yaml
inputs:
  partition_value: "{{sys.execution_date}}"
```

使用系统变量获取执行日期

### 3. 补跑支持

```yaml
catchup: true
```

历史缺失的日期自动补跑

### 4. 幂等性

```yaml
write_mode: "overwrite"
```

重跑同一天数据结果一致

### 5. 分区验证

```yaml
condition: "{{extract_increment.partition_exists == true}}"
```

确保上游分区存在

---

## 运行时行为

### 正常执行

```text
2025-12-04 02:00 触发（处理 2025-12-03 数据）
  ↓
[02:00] 提取 ds=20251203 分区（10 分钟）
  ↓
[02:10] 去重清洗（15 分钟）
  ↓
[02:25] 业务聚合（20 分钟）
  ↓
[02:45] 写入分区 ds=20251203（10 分钟）
  ↓
[02:55] 数据验证（5 分钟）
  ↓
[03:00] 完成

总耗时：1 小时
```

### 补跑场景

```text
假设 2025-12-01 到 2025-12-03 数据缺失

启用 catchup 后：
- 2025-12-04 02:00 触发 3 次执行
  - Run 1: execution_date = 2025-12-01
  - Run 2: execution_date = 2025-12-02
  - Run 3: execution_date = 2025-12-03
- 每次执行独立，按时间顺序串行
```

### 重跑场景

```text
2025-12-03 的数据有问题，需要重新处理

手动触发：
  execution_date = 2025-12-03
  ↓
重新提取 ds=20251203 分区
  ↓
...（相同流程）
  ↓
覆盖写入 ds=20251203
  ↓
结果与首次执行一致（幂等性）
```

---

## 监控指标

| 指标 | 阈值 | 说明 |
|------|------|------|
| 执行成功率 | > 99% | 每日稳定性 |
| 执行时长 | < 90 分钟 | 需在凌晨 4:00 前完成 |
| 数据量波动 | ± 30% | 与历史均值对比 |
| 分区完整性 | 100% | 无缺失日期 |
| 数据质量 | > 95% | 验证通过率 |

---

## 与场景 1 对比

| 维度 | 场景 1（批量特征工程） | 场景 2（增量数据处理） |
|------|----------------------|----------------------|
| 触发方式 | 手动/临时 | **定时自动（cron）** |
| 数据范围 | 全量（7 天） | **增量（1 天）** |
| 时间概念 | 无 | **execution_date** |
| 数据量 | 500GB | 30GB |
| 执行频率 | 每周一次 | **每天一次** |
| 补跑需求 | 重新执行 | **catchup 自动补跑** |
| 幂等性 | 不强制 | **必须保证** |

---

## 总结

这是一个**典型的增量 ETL 场景**，验证了：

1. **scheduled 执行模式**：cron 定时调度
2. **execution_date**：时间分区处理
3. **catchup 机制**：自动补跑历史
4. **幂等性**：重跑结果一致
5. **分区验证**：确保上游数据完整
