# 场景 5：用户行为分析与推荐

## 场景概述

**业务需求**：实时分析用户行为，批量生成推荐特征，支持模型训练与在线推理

**执行模式**：流批一体（实时流 + 批处理 + 人工审批）

**核心特点**：
- 流式实时处理用户点击流
- 批量特征工程与模型训练
- 条件分支与多路汇聚
- 外部事件触发（数据到达、配置变更、人工审批）
- 复杂依赖关系（顺序、并行、条件）

---

## 整体架构

```text
┌─────────────────────── 实时流处理链路 ───────────────────────┐
│                                                              │
│  Kafka用户点击流 → 实时清洗 → 实时聚合 → Redis特征缓存         │
│    (streaming)      (streaming)  (streaming)     (持续更新)  │
│                                                              │
└──────────────────────────────────────────────────────────────┘

                              ↓ (每日凌晨2点触发)

┌─────────────────────── 批量特征工程链路 ───────────────────────┐
│                                                               │
│  提取历史数据(7天) → 数据清洗 → [并行]                         │
│   (batch)          (batch)      ├→ 用户特征工程               │
│                                 │   (batch)                   │
│                                 ├→ 物品特征工程               │
│                                 │   (batch)                   │
│                                 └→ 交叉特征工程               │
│                                     (batch)                   │
│                                         ↓ (汇聚)              │
│                                    特征合并验证                │
│                                     (batch)                   │
│                                         ↓                     │
│                                  [条件分支]                    │
│                      质量分≥0.9 ─────┼───── 质量分<0.9        │
│                           │          │           │            │
│                           ↓          │           ↓            │
│                      自动入库        │      人工审批           │
│                      (batch)         │      (approval)        │
│                           │          │           │            │
│                           │          │       审批通过          │
│                           │          │           ↓            │
│                           │          │      审批后入库         │
│                           │          │      (batch)           │
│                           │          │           │            │
│                           └──────────┴───────────┘            │
│                                      ↓                        │
│                                 模型训练                       │
│                                 (pyspark)                     │
│                                      ↓                        │
│                               模型评估与部署                   │
│                                 (approval)                    │
│                                                               │
└───────────────────────────────────────────────────────────────┘

                              ↓ (部署审批通过)

┌──────────────────────── 模型服务链路 ─────────────────────────┐
│                                                               │
│  外部事件：predict_request.received                           │
│       ↓                                                       │
│  模型推理服务 → 推理结果写入 → 推荐结果入库                     │
│  (streaming)    (batch)       (batch)                        │
│                                                               │
└───────────────────────────────────────────────────────────────┘
```

---

## 数据流详细设计

### 第一部分：实时流处理（持续运行）

#### 1. Kafka 用户点击流摄入

**节点 ID**: `ingest_clickstream`

**类型**: PySparkOperator (streaming)

**输入**:
- Kafka Topic: `user_clicks`
- 配置: `{{ pipeline.input.kafka_config }}`

**处理**:
```python
spark.readStream
  .format("kafka")
  .option("kafka.bootstrap.servers", kafka_config)
  .option("subscribe", "user_clicks")
  .option("startingOffsets", "latest")
  .load()
```

**输出**:
- `raw_stream`: 原始点击流 DataFrame

**启动条件**: `startWhen: "event:pipeline.started"`

**停止条件**: `stopWhen: "event:manual.stop || event:pipeline.shutdown"`

**重启条件**: `restartWhen: "event:kafka_config.updated"`

---

#### 2. 实时数据清洗

**节点 ID**: `clean_clickstream`

**类型**: PySparkOperator (streaming)

**输入**:
- `click_stream`: `{{ ingest_clickstream.raw_stream }}`

**处理**:
- 去除测试用户
- 过滤无效点击（停留时间<1s）
- 字段标准化
- 添加处理时间戳

**输出**:
- `cleaned_stream`: 清洗后的点击流

**启动条件**: `startWhen: "event:ingest_clickstream.started"`

**停止条件**: `stopWhen: "event:ingest_clickstream.stopped"`

---

#### 3. 实时特征聚合

**节点 ID**: `realtime_aggregation`

**类型**: PySparkOperator (streaming)

**输入**:
- `stream`: `{{ clean_clickstream.cleaned_stream }}`
- `window_duration`: `"5 minutes"`

**处理**:
```python
# 5分钟滚动窗口聚合
stream.groupBy(
  window("event_time", "5 minutes"),
  "user_id"
).agg(
  count("*").alias("click_count"),
  countDistinct("item_id").alias("unique_items"),
  avg("dwell_time").alias("avg_dwell_time")
)
```

**输出**:
- `aggregated_features`: 实时特征流 → 写入 Redis

**启动条件**: `startWhen: "event:clean_clickstream.started"`

**停止条件**: `stopWhen: "event:clean_clickstream.stopped"`

**告警条件**: `alertWhen: "cron:*/5 * * * * && {{ lag_seconds > 300 }}"` (延迟超5分钟告警)

---

### 第二部分：批量特征工程（每日触发）

#### 4. 提取历史数据

**节点 ID**: `extract_historical_data`

**类型**: SQLOperator (batch)

**输入**:
- `start_date`: `{{ now() - 7d }}`
- `end_date`: `{{ now() }}`

**处理**:
```sql
SELECT *
FROM warehouse.user_behavior_log
WHERE dt BETWEEN '${start_date}' AND '${end_date}'
  AND is_test_user = false
```

**输出**:
- `historical_data`: 历史数据表路径
- `row_count`: 数据行数

**启动条件**: `startWhen: "cron:0 2 * * *"` (每日凌晨2点)

**重试条件**: `retryWhen: "event:extract_historical_data.failed && {{ attempts < 3 }}"`

---

#### 5. 批量数据清洗

**节点 ID**: `batch_clean_data`

**类型**: PySparkOperator (batch)

**输入**:
- `raw_data`: `{{ extract_historical_data.historical_data }}`

**处理**:
- 去重
- 异常值过滤
- 数据归一化

**输出**:
- `cleaned_data`: 清洗后数据路径
- `cleaning_report`: 清洗报告（去重率、过滤率）

**启动条件**: `startWhen: "event:extract_historical_data.completed"`

---

#### 6a. 用户特征工程（并行任务1）

**节点 ID**: `user_feature_engineering`

**类型**: PySparkOperator (batch)

**输入**:
- `cleaned_data`: `{{ batch_clean_data.cleaned_data }}`

**处理**:
- 用户点击频次统计（7天、3天、1天）
- 用户活跃时段分布
- 用户偏好类目 Top-K
- 用户生命周期特征

**输出**:
- `user_features`: 用户特征表
- `feature_count`: 特征数量

**启动条件**: `startWhen: "event:batch_clean_data.completed"`

---

#### 6b. 物品特征工程（并行任务2）

**节点 ID**: `item_feature_engineering`

**类型**: PySparkOperator (batch)

**输入**:
- `cleaned_data`: `{{ batch_clean_data.cleaned_data }}`

**处理**:
- 物品点击率/转化率
- 物品协同过滤特征
- 物品类目编码
- 物品流行度趋势

**输出**:
- `item_features`: 物品特征表

**启动条件**: `startWhen: "event:batch_clean_data.completed"`

---

#### 6c. 交叉特征工程（并行任务3）

**节点 ID**: `cross_feature_engineering`

**类型**: PySparkOperator (batch)

**输入**:
- `cleaned_data`: `{{ batch_clean_data.cleaned_data }}`

**处理**:
- 用户-物品交互历史
- 用户-类目偏好匹配度
- 时间窗口交叉特征

**输出**:
- `cross_features`: 交叉特征表

**启动条件**: `startWhen: "event:batch_clean_data.completed"`

---

#### 7. 特征合并与验证（多路汇聚）

**节点 ID**: `merge_and_validate_features`

**类型**: PySparkOperator (batch)

**输入**:
- `user_features`: `{{ user_feature_engineering.user_features }}`
- `item_features`: `{{ item_feature_engineering.item_features }}`
- `cross_features`: `{{ cross_feature_engineering.cross_features }}`

**处理**:
- 按 user_id, item_id 关联合并
- 数据质量检查（覆盖率、缺失率、分布异常）
- 计算质量得分

**输出**:
- `merged_features`: 合并后特征表
- `quality_score`: 质量得分 (0-1)
- `validation_report`: 验证报告

**启动条件**: 
```
startWhen: "event:user_feature_engineering.completed && 
            event:item_feature_engineering.completed && 
            event:cross_feature_engineering.completed"
```

---

### 第三部分：条件分支与审批

#### 8a. 自动入库（高质量分支）

**节点 ID**: `auto_load_features`

**类型**: SQLOperator (batch)

**输入**:
- `features`: `{{ merge_and_validate_features.merged_features }}`

**处理**:
```sql
INSERT OVERWRITE TABLE feature_store.user_item_features
SELECT * FROM ${features}
```

**输出**:
- `load_status`: "success"

**启动条件**: 
```
startWhen: "event:merge_and_validate_features.completed && 
            {{ merge_and_validate_features.quality_score >= 0.9 }}"
```

---

#### 8b. 人工审批（低质量分支）

**节点 ID**: `manual_review_features`

**类型**: Approval

**输入**:
- `quality_score`: `{{ merge_and_validate_features.quality_score }}`
- `validation_report`: `{{ merge_and_validate_features.validation_report }}`
- `approvers`: `["data-quality-team@company.com"]`

**审批内容**:
```
特征质量得分: {{ quality_score }}
异常详情: {{ validation_report }}
是否允许入库？
```

**输出**:
- `approval_result`: "approved" | "rejected"
- `approval_comment`: 审批意见

**启动条件**: 
```
startWhen: "event:merge_and_validate_features.completed && 
            {{ merge_and_validate_features.quality_score < 0.9 }}"
```

**超时处理**: `timeout: "2h"`, `onTimeout: "auto_reject"`

---

#### 8c. 审批后入库

**节点 ID**: `load_features_after_approval`

**类型**: SQLOperator (batch)

**输入**:
- `features`: `{{ merge_and_validate_features.merged_features }}`

**处理**:
```sql
INSERT OVERWRITE TABLE feature_store.user_item_features
SELECT * FROM ${features}
```

**输出**:
- `load_status`: "success"

**启动条件**: 
```
startWhen: "event:manual_review_features.completed && 
            {{ manual_review_features.approval_result == 'approved' }}"
```

---

#### 9. 模型训练（多路汇聚后触发）

**节点 ID**: `train_recommendation_model`

**类型**: PySparkOperator (batch)

**输入**:
- `feature_table`: `"feature_store.user_item_features"`
- `algorithm`: `"collaborative_filtering"`
- `rank`: `50`
- `max_iter`: `20`

**处理**:
```python
from pyspark.ml.recommendation import ALS

als = ALS(
    rank=rank,
    maxIter=max_iter,
    userCol="user_id",
    itemCol="item_id",
    ratingCol="rating"
)

model = als.fit(training_data)
```

**输出**:
- `model_path`: 模型存储路径
- `training_metrics`: 训练指标（RMSE, MAE）

**启动条件**: 
```
startWhen: "event:auto_load_features.completed || 
            event:load_features_after_approval.completed"
```

**重试条件**: `retryWhen: "event:train_recommendation_model.failed && {{ attempts < 2 }}"`

---

#### 10. 模型评估与部署审批

**节点 ID**: `evaluate_and_approve_model`

**类型**: Approval

**输入**:
- `model_path`: `{{ train_recommendation_model.model_path }}`
- `training_metrics`: `{{ train_recommendation_model.training_metrics }}`
- `approvers`: `["ml-team@company.com"]`

**审批内容**:
```
模型训练完成
RMSE: {{ training_metrics.rmse }}
MAE: {{ training_metrics.mae }}
是否部署到生产环境？
```

**输出**:
- `deployment_approved`: true | false
- `model_version`: 部署版本号

**启动条件**: `startWhen: "event:train_recommendation_model.completed"`

---

### 第四部分：模型推理服务（外部事件触发）

#### 11. 模型推理服务

**节点 ID**: `model_inference_service`

**类型**: ModelInference (streaming)

**输入**:
- `model_version`: `{{ evaluate_and_approve_model.model_version }}`
- `event_source`: Kafka Topic `predict_requests`

**处理**:
- 监听预测请求事件
- 从 Redis 加载实时特征
- 调用推荐模型推理
- 返回推荐结果

**输出**:
- `prediction_stream`: 推理结果流

**启动条件**: 
```
startWhen: "event:evaluate_and_approve_model.completed && 
            {{ evaluate_and_approve_model.deployment_approved == true }}"
```

**停止条件**: `stopWhen: "event:model.deprecated || event:manual.stop"`

**重启条件**: `restartWhen: "event:model.updated"` (模型更新后重启)

---

#### 12. 推理结果落库

**节点 ID**: `store_prediction_results`

**类型**: SQLOperator (batch)

**输入**:
- `predictions`: `{{ model_inference_service.prediction_stream }}`
- `batch_size`: `10000`

**处理**:
```sql
INSERT INTO recommendation_log.predictions
SELECT 
  user_id,
  item_id,
  score,
  predicted_at
FROM ${predictions}
```

**输出**:
- `stored_count`: 存储记录数

**启动条件**: `startWhen: "event:model_inference_service.batch_ready"`

**启动模式**: `startMode: "repeat"` (持续批量写入)

---

## Pipeline 定义（YAML 表示）

```yaml
pipelineDefinition:
  id: "user-behavior-analysis-recommendation"
  namespace: "com.company.recommendation"
  name: "用户行为分析与推荐"
  
  versions:
    - version: "1.0.0"
      status: "PUBLISHED"
      
      inputVariables:
        - name: "kafka_config"
          type: "string"
          description: "Kafka集群配置"
          defaultValue: "localhost:9092"
      
      outputVariables:
        - name: "model_version"
          type: "string"
          source: "{{ evaluate_and_approve_model.model_version }}"
        - name: "feature_quality_score"
          type: "double"
          source: "{{ merge_and_validate_features.quality_score }}"
      
      nodes:
        # ========== 实时流处理链路 ==========
        - id: "ingest_clickstream"
          type: "task"
          taskDefinition:
            ref: "com.company.tasks:pyspark_streaming:1.0.0"
          inputBindings:
            kafka_bootstrap_servers: "{{ pipeline.input.kafka_config }}"
            kafka_topic: "user_clicks"
            starting_offsets: "latest"
          startWhen: "event:pipeline.started"
          startMode: "once"
          stopWhen: "event:manual.stop || event:pipeline.shutdown"
          restartWhen: "event:kafka_config.updated"
        
        - id: "clean_clickstream"
          type: "task"
          taskDefinition:
            ref: "com.company.tasks:pyspark_streaming:1.0.0"
          inputBindings:
            input_stream: "{{ ingest_clickstream.raw_stream }}"
            operation: "clean"
          startWhen: "event:ingest_clickstream.started"
          stopWhen: "event:ingest_clickstream.stopped"
        
        - id: "realtime_aggregation"
          type: "task"
          taskDefinition:
            ref: "com.company.tasks:pyspark_streaming:1.0.0"
          inputBindings:
            input_stream: "{{ clean_clickstream.cleaned_stream }}"
            window_duration: "5 minutes"
            output_sink: "redis://features"
          startWhen: "event:clean_clickstream.started"
          stopWhen: "event:clean_clickstream.stopped"
          alertWhen: "cron:*/5 * * * * && {{ lag_seconds > 300 }}"
        
        # ========== 批量特征工程链路 ==========
        - id: "extract_historical_data"
          type: "task"
          taskDefinition:
            ref: "com.company.tasks:sql_query:1.0.0"
          inputBindings:
            query: |
              SELECT * FROM warehouse.user_behavior_log
              WHERE dt BETWEEN '{{ now() - 7d }}' AND '{{ now() }}'
                AND is_test_user = false
          startWhen: "cron:0 2 * * *"
          startMode: "once"
          retryWhen: "event:extract_historical_data.failed && {{ attempts < 3 }}"
        
        - id: "batch_clean_data"
          type: "task"
          taskDefinition:
            ref: "com.company.tasks:pyspark_batch:1.0.0"
          inputBindings:
            input_data: "{{ extract_historical_data.historical_data }}"
            operation: "clean_batch"
          startWhen: "event:extract_historical_data.completed"
        
        # 并行特征工程
        - id: "user_feature_engineering"
          type: "task"
          taskDefinition:
            ref: "com.company.tasks:pyspark_batch:1.0.0"
          inputBindings:
            input_data: "{{ batch_clean_data.cleaned_data }}"
            feature_type: "user"
          startWhen: "event:batch_clean_data.completed"
        
        - id: "item_feature_engineering"
          type: "task"
          taskDefinition:
            ref: "com.company.tasks:pyspark_batch:1.0.0"
          inputBindings:
            input_data: "{{ batch_clean_data.cleaned_data }}"
            feature_type: "item"
          startWhen: "event:batch_clean_data.completed"
        
        - id: "cross_feature_engineering"
          type: "task"
          taskDefinition:
            ref: "com.company.tasks:pyspark_batch:1.0.0"
          inputBindings:
            input_data: "{{ batch_clean_data.cleaned_data }}"
            feature_type: "cross"
          startWhen: "event:batch_clean_data.completed"
        
        # 多路汇聚
        - id: "merge_and_validate_features"
          type: "task"
          taskDefinition:
            ref: "com.company.tasks:pyspark_batch:1.0.0"
          inputBindings:
            user_features: "{{ user_feature_engineering.user_features }}"
            item_features: "{{ item_feature_engineering.item_features }}"
            cross_features: "{{ cross_feature_engineering.cross_features }}"
            operation: "merge_validate"
          startWhen: |
            event:user_feature_engineering.completed && 
            event:item_feature_engineering.completed && 
            event:cross_feature_engineering.completed
        
        # ========== 条件分支与审批 ==========
        # 高质量自动入库
        - id: "auto_load_features"
          type: "task"
          taskDefinition:
            ref: "com.company.tasks:sql_load:1.0.0"
          inputBindings:
            source_table: "{{ merge_and_validate_features.merged_features }}"
            target_table: "feature_store.user_item_features"
          startWhen: |
            event:merge_and_validate_features.completed && 
            {{ merge_and_validate_features.quality_score >= 0.9 }}
        
        # 低质量需审批
        - id: "manual_review_features"
          type: "task"
          taskDefinition:
            ref: "com.company.tasks:approval:1.0.0"
          inputBindings:
            title: "特征质量审批"
            description: |
              质量得分: {{ merge_and_validate_features.quality_score }}
              验证报告: {{ merge_and_validate_features.validation_report }}
            approvers: ["data-quality-team@company.com"]
            timeout: "2h"
            on_timeout: "auto_reject"
          startWhen: |
            event:merge_and_validate_features.completed && 
            {{ merge_and_validate_features.quality_score < 0.9 }}
        
        # 审批后入库
        - id: "load_features_after_approval"
          type: "task"
          taskDefinition:
            ref: "com.company.tasks:sql_load:1.0.0"
          inputBindings:
            source_table: "{{ merge_and_validate_features.merged_features }}"
            target_table: "feature_store.user_item_features"
          startWhen: |
            event:manual_review_features.completed && 
            {{ manual_review_features.approval_result == 'approved' }}
        
        # ========== 模型训练 ==========
        # 多路汇聚：无论是自动入库还是审批后入库，都触发模型训练
        - id: "train_recommendation_model"
          type: "task"
          taskDefinition:
            ref: "com.company.tasks:pyspark_ml:1.0.0"
          inputBindings:
            feature_table: "feature_store.user_item_features"
            algorithm: "ALS"
            rank: 50
            max_iter: 20
          startWhen: |
            event:auto_load_features.completed || 
            event:load_features_after_approval.completed
          retryWhen: "event:train_recommendation_model.failed && {{ attempts < 2 }}"
        
        # 模型部署审批
        - id: "evaluate_and_approve_model"
          type: "task"
          taskDefinition:
            ref: "com.company.tasks:approval:1.0.0"
          inputBindings:
            title: "模型部署审批"
            description: |
              RMSE: {{ train_recommendation_model.training_metrics.rmse }}
              MAE: {{ train_recommendation_model.training_metrics.mae }}
            approvers: ["ml-team@company.com"]
          startWhen: "event:train_recommendation_model.completed"
        
        # ========== 模型推理服务 ==========
        - id: "model_inference_service"
          type: "task"
          taskDefinition:
            ref: "com.company.tasks:model_inference:1.0.0"
          inputBindings:
            model_path: "{{ train_recommendation_model.model_path }}"
            model_version: "{{ evaluate_and_approve_model.model_version }}"
            event_source: "kafka://predict_requests"
            feature_store: "redis://features"
          startWhen: |
            event:evaluate_and_approve_model.completed && 
            {{ evaluate_and_approve_model.deployment_approved == true }}
          startMode: "once"
          stopWhen: "event:model.deprecated || event:manual.stop"
          restartWhen: "event:model.updated"
        
        # 推理结果落库
        - id: "store_prediction_results"
          type: "task"
          taskDefinition:
            ref: "com.company.tasks:sql_load:1.0.0"
          inputBindings:
            source_stream: "{{ model_inference_service.prediction_stream }}"
            target_table: "recommendation_log.predictions"
            batch_size: 10000
          startWhen: "event:model_inference_service.batch_ready"
          startMode: "repeat"
      
      releaseNotes: "初始版本：支持流批一体推荐系统"
      createdAt: "2025-12-09T00:00:00Z"
      createdBy: "system"
```

---

## 核心设计亮点

### 1. **流批一体架构**
- **实时流**: `ingest_clickstream` → `clean_clickstream` → `realtime_aggregation`
- **批量处理**: `extract_historical_data` → 特征工程 → 模型训练
- 两条链路独立运行，批量任务每日定时触发

### 2. **复杂依赖关系**

#### 并行执行
```
batch_clean_data 
    ↓
    ├→ user_feature_engineering
    ├→ item_feature_engineering  (三者并行)
    └→ cross_feature_engineering
```

#### 多路汇聚（AND）
```
merge_and_validate_features 需等待三个特征工程任务全部完成
startWhen: "event:user_feature_engineering.completed && 
            event:item_feature_engineering.completed && 
            event:cross_feature_engineering.completed"
```

#### 条件分支
```
                merge_and_validate_features
                         ↓
        ┌────────────────┴────────────────┐
        │                                 │
质量分≥0.9                          质量分<0.9
        ↓                                 ↓
auto_load_features              manual_review_features
                                         ↓
                          load_features_after_approval
```

#### 多路汇聚（OR）
```
train_recommendation_model 在任一入库完成后触发
startWhen: "event:auto_load_features.completed || 
            event:load_features_after_approval.completed"
```

### 3. **外部事件触发**

- **定时事件**: `cron:0 2 * * *` (每日凌晨2点)
- **配置变更**: `event:kafka_config.updated` → 重启流任务
- **人工审批**: `manual_review_features` → 审批通过后继续
- **外部请求**: `event:predict_request.received` → 模型推理
- **模型更新**: `event:model.updated` → 重启推理服务

### 4. **Approval 审批节点**

- **特征质量审批**: 质量分<0.9时触发人工审批
- **模型部署审批**: 训练完成后需ML团队审批才能部署
- **超时处理**: 2小时未审批自动拒绝

### 5. **流式任务生命周期管理**

- `startWhen`: 何时启动
- `stopWhen`: 何时停止 (manual.stop、pipeline.shutdown)
- `restartWhen`: 何时重启 (配置更新、模型更新)
- `alertWhen`: 延迟监控告警

### 6. **数据依赖传递**

所有节点通过 `{{ node_id.variable_name }}` 显式声明数据依赖：
```yaml
inputBindings:
  user_features: "{{ user_feature_engineering.user_features }}"
  item_features: "{{ item_feature_engineering.item_features }}"
```

---

## 执行时序图

```
T0: Pipeline 启动
├─ ingest_clickstream (持续运行)
│   └─ clean_clickstream (持续运行)
│       └─ realtime_aggregation (持续运行，写Redis)
│
T1: 凌晨2点 (cron触发)
├─ extract_historical_data (30分钟)
│   └─ batch_clean_data (1小时)
│       ├─ user_feature_engineering (1.5小时)
│       ├─ item_feature_engineering (1小时)
│       └─ cross_feature_engineering (1小时)
│           └─ [汇聚] merge_and_validate_features (30分钟)
│               ├─ [质量分≥0.9] auto_load_features (20分钟)
│               │   └─ train_recommendation_model (2小时)
│               │       └─ evaluate_and_approve_model (等待人工审批)
│               │           └─ model_inference_service (持续运行)
│               │               └─ store_prediction_results (持续批量写入)
│               │
│               └─ [质量分<0.9] manual_review_features (等待审批)
│                   └─ [审批通过] load_features_after_approval (20分钟)
│                       └─ train_recommendation_model (2小时)
│                           └─ ...
│
时间轴: |--1h--|--2h--|--3h--|--4h--|--5h--|--6h--|--审批--|--推理服务-->
```

---

## 监控指标

- **实时流延迟**: `realtime_aggregation.lag_seconds < 300s`
- **批量任务执行时长**: `extract_historical_data.duration < 30min`
- **特征质量得分**: `merge_and_validate_features.quality_score >= 0.9`
- **模型训练指标**: `train_recommendation_model.rmse < 0.5`
- **审批响应时间**: `manual_review_features.pending_duration`
- **推理服务QPS**: `model_inference_service.qps`

---

## 总结

该 Pipeline 综合展示了：

✅ **流批一体**: 实时流 + 批量处理混合编排  
✅ **复杂依赖**: 顺序、并行、条件分支、多路汇聚  
✅ **外部事件**: 定时触发、配置变更、人工审批、外部请求  
✅ **生命周期管理**: streaming 任务的启动/停止/重启控制  
✅ **数据质量保障**: 自动检测 + 人工审批双重保障  
✅ **模型全流程**: 特征工程 → 模型训练 → 审批部署 → 在线推理  

这是一个**生产级的、典型的、完整的**数据处理管线样例。
