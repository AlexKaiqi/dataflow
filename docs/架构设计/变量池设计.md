# 变量池设计

## 1. 概念定位

### 1.1 核心抽象

**变量池**是流水线执行过程中的**数据交换中心**。

**职责**：

- 存储和查询变量
- 支持表达式求值
- 管理变量命名空间
- 处理大对象引用

**不负责**：

- ❌ 不管理变量生命周期（由编排引擎管理）
- ❌ 不做访问权限控制
- ❌ 不做永久持久化（只是执行期缓存）

---

## 2. 命名空间设计

### 2.1 命名空间结构

**变量通过分层命名空间组织**：

```
{namespace}.{name}

命名空间类型:
  - pipeline.input.*     # 流水线输入参数
  - {node_id}.*          # 节点输出变量
  - system.*             # 系统变量
```

**示例**：

```
pipeline.input.start_date       # 流水线输入：开始日期
pipeline.input.end_date         # 流水线输入：结束日期
extract_data.output_path        # 节点输出：数据路径
extract_data.row_count          # 节点输出：行数
system.execution_id             # 系统变量：执行 ID
system.started_at               # 系统变量：启动时间
```

### 2.2 隔离机制

**变量隔离靠 execution_id 实现**：

```
变量完整标识 = (execution_id, variable_name)

示例:
  (exec_12345, "pipeline.input.start_date")
  (exec_12345, "extract_data.row_count")
  
不同执行实例完全隔离:
  (exec_12345, "extract_data.row_count") = 1000000
  (exec_67890, "extract_data.row_count") = 2000000
```

**隔离层次**：

```
第一层：execution_id 隔离
  └─ execution_12345
      ├─ 第二层：命名空间隔离
      │   ├─ pipeline.input.*
      │   ├─ node_a.*
      │   └─ node_b.*
      │
  └─ execution_67890（完全独立）
  pipeline.input.start_date: "2025-02-01"
  extract_data.row_count: 2000000
```

**隔离效果**：

- 支持同一流水线的并发执行
- 避免不同执行实例的变量冲突
- 简化变量生命周期管理

### 2.3 可见性规则

**核心原则**：在同一 execution_id 内，变量全局共享

#### 规则 1: 始终可见的变量

| 命名空间 | 何时可见 | 示例 |
|---------|---------|------|
| `pipeline.input.*` | 执行开始时 | `pipeline.input.start_date` |
| `system.*` | 执行开始时 | `system.execution_id` |

#### 规则 2: 动态可见的变量

**节点输出变量**在节点完成后可见：

```
变量可见性流程:
  节点执行完成
    ↓
  输出变量写入变量池
    ↓
  此后任何节点都可以访问该变量

关键点:
  - ✅ 基于变量是否存在（运行时判断）
  - ❌ 不基于 DAG 拓扑（无预定义依赖）
  - 执行顺序由事件表达式（startWhen）决定
```

#### 规则 3: 访问限制

| 场景 | 是否允许 | 说明 |
|------|---------|------|
| 访问已存在的变量 | ✅ | 查询变量池，返回值 |
| 访问不存在的变量 | ❌ | 抛出 VARIABLE_NOT_FOUND |
| 跨执行实例访问 | ❌ | execution_id 隔离 |
| 访问子流水线内部变量 | ❌ | 独立执行空间隔离 |

### 2.4 访问检查流程

```
节点触发:
  编排引擎解析 inputBindings
    ↓
  提取变量引用: "{{ extract_data.output_path }}"
    ↓
  查询变量池:
    exists(execution_id, "extract_data.output_path")?
    ↓
  存在 → 返回变量值 ✅
  不存在 → 抛出错误 ❌
```

---

## 3. 变量访问示例

### 3.1 场景：批量特征工程流水线

**场景：批量特征工程流水线**

```yaml
Pipeline:
  inputVariables:
    - start_date
    - end_date
  
  nodes:
    - id: extract_data
      startWhen: "event:pipeline.started"
      # 可见变量：
      #   - pipeline.input.start_date ✅
      #   - pipeline.input.end_date ✅
      #   - system.execution_id ✅
      inputBindings:
        start_date: "{{ pipeline.input.start_date }}"
        end_date: "{{ pipeline.input.end_date }}"
    
    - id: transform_data
      startWhen: "event:extract_data.completed"
      # 此时变量池中已有：
      #   - pipeline.input.* ✅
      #   - system.* ✅
      #   - extract_data.* ✅（extract_data 已完成，变量已写入）
      # 此时变量池中没有：
      #   - quality_check.* ❌（节点尚未执行）
      inputBindings:
        input_path: "{{ extract_data.output_path }}"
        row_count: "{{ extract_data.row_count }}"
    
    - id: quality_check
      startWhen: "event:transform_data.completed"
      # 此时变量池中已有：
      #   - pipeline.input.* ✅
      #   - system.* ✅
      #   - extract_data.* ✅
      #   - transform_data.* ✅
      inputBindings:
        data_path: "{{ transform_data.output_path }}"
        threshold: 0.9
```

**变量访问示例**：

```
# transform_data 节点触发时（extract_data 已完成）
变量池状态:
  pipeline.input.start_date: "2025-01-01"
  pipeline.input.end_date: "2025-01-31"
  system.execution_id: "exec_12345"
  extract_data.output_path: "s3://bucket/data.parquet"  ✅ 已存在
  extract_data.row_count: 1000000  ✅ 已存在

编排引擎解析 inputBindings:
  "{{ extract_data.output_path }}"
  ↓
变量池查询: extract_data.output_path
  → 找到 ✅ 返回 "s3://bucket/data.parquet"

# 如果 transform_data 试图访问 quality_check 的输出（错误示例）
  "{{ quality_check.score }}"
  ↓
变量池查询: quality_check.score
  → 不存在 ❌ 抛出错误: VARIABLE_NOT_FOUND
```

### 2.2 变量类型

支持的变量类型：

| 类型 | 说明 | 存储方式 | 示例 |
|------|------|---------|------|
| `string` | 字符串 | 直接存储 | `"2025-01-01"` |
| `number` | 数字（整数/浮点数） | 直接存储 | `1000000`, `0.95` |
| `boolean` | 布尔值 | 直接存储 | `true`, `false` |
| `object` | JSON 对象 | 直接存储（小对象）| `{"key": "value"}` |
| `array` | 数组 | 直接存储（小数组） | `[1, 2, 3]` |
| `file` | 文件路径 | 存储引用 | `"s3://bucket/file.csv"` |
| `dataframe` | DataFrame | 存储元数据 | `{"path": "s3://...", "schema": {...}}` |

### 2.3 子流水线的变量隔离

当节点类型为 `pipeline`（引用子流水线）时，变量隔离更加重要。

#### 2.3.1 子流水线变量空间

子流水线拥有**独立的变量空间**：

```
父流水线 (execution_parent):
  pipeline.input.start_date: "2025-01-01"
  extract_data.row_count: 1000000
  
  ├─ 子流水线节点: sub_etl
  │   └─ 子流水线执行 (execution_child):
  │       pipeline.input.source_table: "user_events"  # 独立的输入
  │       transform.output_path: "s3://..."           # 独立的输出
  │       不能访问父流水线的 extract_data.row_count ❌

父子流水线变量完全隔离
```

#### 2.3.2 变量传递机制

**输入传递**：父流水线通过 `inputBindings` 向子流水线传递输入

```yaml
父流水线:
  nodes:
    - id: sub_etl
      type: pipeline
      pipelineDefinition:
        ref: "com.company:etl_pipeline:1.0.0"
      inputBindings:
        # 父流水线变量 → 子流水线输入
        source_table: "{{ pipeline.input.table_name }}"
        date_range: "{{ extract_data.date_range }}"
      startWhen: "event:extract_data.completed"

数据流向:
  父: pipeline.input.table_name → 子: pipeline.input.source_table
  父: extract_data.date_range   → 子: pipeline.input.date_range
```

**输出传递**：子流水线完成后，输出变量写入父流水线的节点作用域

```yaml
子流水线定义:
  outputVariables:
    - name: output_path
    - name: record_count

子流水线执行完成后:
  父流水线变量池写入:
    sub_etl.output_path: "s3://bucket/result.parquet"
    sub_etl.record_count: 5000000

后续节点可以访问:
  - id: quality_check
    inputBindings:
      data_path: "{{ sub_etl.output_path }}"
      expected_count: "{{ sub_etl.record_count }}"
```

#### 2.3.3 子流水线访问规则

| 访问方向 | 规则 | 说明 |
|---------|------|------|
| **子 → 父** | ❌ 禁止 | 子流水线不能访问父流水线的变量 |
| **父 → 子输入** | ✅ 通过 inputBindings | 父流水线通过参数传递方式提供输入 |
| **父 → 子输出** | ✅ 通过 outputVariables | 子流水线完成后，输出写入父流水线节点作用域 |
| **子 → 外部** | ❌ 禁止 | 子流水线不能访问其他流水线的变量 |

**设计原因**：

- **封装性**：子流水线是独立的可复用单元，不应依赖父流水线的内部变量
- **可测试性**：子流水线可以独立测试，不需要模拟父流水线环境
- **清晰接口**：通过 inputVariables 和 outputVariables 明确定义接口契约

#### 2.3.4 示例：嵌套流水线的变量隔离

```yaml
# 父流水线
PipelineDefinition:
  name: "main_pipeline"
  
  inputVariables:
    - name: batch_date
  
  nodes:
    - id: prepare_data
      type: task
      startWhen: "event:pipeline.started"
      # 输出: prepare_data.input_path, prepare_data.record_count
    
    - id: sub_processing
      type: pipeline
      pipelineDefinition:
        ref: "com.company:data_processing:1.0.0"
      inputBindings:
        # 只传递必要的输入
        input_file: "{{ prepare_data.input_path }}"
        batch_size: 1000
      startWhen: "event:prepare_data.completed"
      # 子流水线内部变量不可见
      # sub_processing 内部的 transform.temp_path 在这里不可访问
    
    - id: validate_result
      type: task
      inputBindings:
        # 可以访问子流水线的输出
        result_path: "{{ sub_processing.output_path }}"
        result_count: "{{ sub_processing.processed_count }}"
        # 不能访问子流水线的内部变量
        # temp_path: "{{ sub_processing.transform.temp_path }}" ❌ 错误
      startWhen: "event:sub_processing.completed"

# 子流水线 (com.company:data_processing:1.0.0)
PipelineDefinition:
  name: "data_processing"
  
  inputVariables:
    - name: input_file
    - name: batch_size
  
  outputVariables:
    - name: output_path
    - name: processed_count
  
  nodes:
    - id: transform
      type: task
      inputBindings:
        input: "{{ pipeline.input.input_file }}"  # 访问子流水线自己的输入
        # 不能访问父流水线的变量
        # date: "{{ pipeline.input.batch_date }}" ❌ 错误（batch_date 是父流水线的输入）
      # 输出: transform.temp_path, transform.output_path
    
    - id: aggregate
      inputBindings:
        input: "{{ transform.output_path }}"  # 访问同级节点的输出 ✅
      # 输出: aggregate.final_path, aggregate.count
```

**变量空间隔离示意**：

```
execution_main (父流水线):
  pipeline.input.batch_date: "2025-01-01"
  prepare_data.input_path: "s3://bucket/input.csv"
  prepare_data.record_count: 1000000
  
  ├─ 子流水线执行 (execution_sub):
  │   pipeline.input.input_file: "s3://bucket/input.csv"  # 从父流水线传入
  │   pipeline.input.batch_size: 1000                     # 从父流水线传入
  │   transform.temp_path: "s3://bucket/temp.parquet"     # 子流水线内部变量
  │   transform.output_path: "s3://bucket/output.parquet"
  │   aggregate.final_path: "s3://bucket/final.parquet"
  │   aggregate.count: 500000
  │   
  │   注意：
  │   - 不能访问 prepare_data.input_path ❌
  │   - 不能访问 batch_date ❌
  
  sub_processing.output_path: "s3://bucket/final.parquet"   # 从子流水线输出写入
  sub_processing.processed_count: 500000                    # 从子流水线输出写入
  
  validate_result.status: "success"
```

### 2.4 大对象处理

对于大对象（DataFrame、大文件），只存储**元数据**而非实际数据：

**DataFrame 变量示例**：

```yaml
name: extract_data.output_df
type: dataframe
value:
  path: "s3://bucket/data/output.parquet"
  format: "parquet"
  schema:
    user_id: "string"
    event_time: "timestamp"
    event_type: "string"
  row_count: 1000000
  partition_keys: ["date"]
metadata:
  storage: "s3"
  size_bytes: 524288000
```

**设计原因**：

- 避免内存溢出
- 支持分布式部署（不同节点访问共享存储）
- 保持变量池轻量级

---

## 3. 核心操作

### 3.1 设置变量（Set Variable）

**操作**：将变量存储到变量池

**输入**：

- 执行实例ID
- 作用域（scope）：`pipeline.input` | `node_id` | `system`
- 变量名（不含作用域前缀）
- 变量值
- 类型提示（可选，可自动推断）
- 元数据（可选）

**处理**：

1. 构建完整变量引用：`{scope}.{name}`
2. 推断或验证类型
3. 存储到变量池

**示例**：

```
设置变量(
  execution_id: "exec_001",
  scope: "extract_data",
  name: "row_count",
  value: 1000000,
  type: "number"
)

→ 存储为: extract_data.row_count = 1000000
```

### 3.2 获取变量（Get Variable）

**操作**：读取变量值

**输入**：

- 执行实例ID
- 变量引用（如 "extract_data.row_count"）

**返回**：变量值

**异常**：变量不存在时抛出错误

**安全版本**：`get_variable_safe` - 不存在时返回默认值

### 3.3 检查变量存在（Has Variable）

**操作**：判断变量是否存在

**输入**：

- 执行实例ID
- 变量引用

**返回**：布尔值（true/false）

### 3.4 解析表达式（Resolve Expression）

**操作**：计算表达式的值

**输入**：

- 执行实例ID
- 表达式（使用 `{{ }}` 语法）

**返回**：表达式求值结果

**支持的表达式**：

| 表达式类型 | 示例 | 说明 |
|-----------|------|------|
| 变量引用 | `{{ extract_data.row_count }}` | 直接访问变量 |
| 算术运算 | `{{ extract_data.row_count + 1000 }}` | 加减乘除 |
| 比较运算 | `{{ extract_data.row_count > 1000 }}` | 大于、小于、等于 |
| 逻辑运算 | `{{ a > 0 && b > 0 }}` | 与、或、非 |
| 嵌套访问 | `{{ extract_data.output_df.row_count }}` | 访问对象属性 |

### 3.5 解析输入绑定（Resolve Bindings）

**操作**：将节点的 `inputBindings` 配置解析为实际值

**输入**：

- 执行实例ID
- 输入绑定配置（字典）

**返回**：解析后的值（字典）

**示例**：

```yaml
# 输入绑定配置
inputBindings:
  input_path: "{{ extract_data.output_path }}"
  quality_threshold: 0.9
  batch_size: "{{ extract_data.row_count / 1000 }}"

# 解析后的值
resolved:
  input_path: "s3://bucket/data/raw.parquet"
  quality_threshold: 0.9
  batch_size: 1000
```

### 3.6 获取所有变量（Get All Variables）

**操作**：返回执行实例的所有变量（用于调试）

**输入**：

- 执行实例ID
- 作用域过滤（可选）

**返回**：变量字典

### 3.7 清理变量（Clear Variables）

**操作**：删除执行实例的所有变量

**输入**：执行实例ID

**用途**：执行完成后清理、测试环境重置

---

## 4. 存储方案

### 4.1 方案1：内存存储（单机部署）

**适用场景**：单机部署、开发测试环境

**存储结构**：

```
变量池:
  execution_12345:
    pipeline.input.start_date:
      value: "2025-01-01"
      type: "string"
      scope: "pipeline.input"
      created_at: "2025-01-15T10:00:00Z"
    
    extract_data.row_count:
      value: 1000000
      type: "number"
      scope: "extract_data"
      created_at: "2025-01-15T10:30:00Z"
```

**特点**：

- 快速访问，低延迟
- 适合单实例部署
- 可选异步持久化到数据库

### 4.2 方案2：分布式存储（生产环境）

**适用场景**：分布式部署、高可用环境

**Redis 存储**：

```
Key: execution:{execution_id}:variables
Type: Hash
Field-Value:
  pipeline.input.start_date → {value: "2025-01-01", type: "string", ...}
  extract_data.row_count → {value: 1000000, type: "number", ...}
```

**分层存储策略**：

| 变量类型 | 存储位置 | 说明 |
|---------|---------|------|
| 小变量（< 1MB） | Redis | 快速访问 |
| 大对象元数据 | Redis | 只存储路径和 schema |
| 大对象数据 | S3/MinIO | 实际数据存储 |

**示例**：

```
# 小变量 - 直接存储
extract_data.row_count = 1000000

# DataFrame - 只存储元数据
extract_data.output_df = {
  "path": "s3://bucket/data/output.parquet",
  "schema": {...},
  "row_count": 1000000
}

# 实际数据
s3://bucket/data/output.parquet ← 实际的 Parquet 文件
```

---

## 5. 与编排引擎的交互

### 5.1 流水线启动时

**流程**：

```
用户启动流水线，提供输入参数
  ↓
编排引擎创建 PipelineExecution
  ↓
编排引擎 → 变量池.设置变量("pipeline.input.start_date", "2025-01-01")
编排引擎 → 变量池.设置变量("pipeline.input.end_date", "2025-01-31")
编排引擎 → 变量池.设置变量("system.execution_id", "exec_12345")
  ↓
变量池初始化完成
```

### 5.2 节点触发时

**流程**：

```
编排引擎判断节点可以执行
  ↓
编排引擎获取节点的 inputBindings 配置
  ↓
编排引擎 → 变量池.解析输入绑定(inputBindings)
  ↓
变量池解析表达式，返回实际值
  ↓
编排引擎将解析后的值传递给执行引擎
```

### 5.3 节点完成时

**流程**：

```
任务执行完成，返回 outputs
  ↓
编排引擎接收 outputs
  ↓
对每个输出变量:
  编排引擎 → 变量池.设置变量(node_id, output_name, value)
  ↓
变量池存储输出变量
  ↓
下游节点可以引用这些变量
```

### 5.4 表达式求值时

**场景1：startWhen 条件判断**

```
节点定义: startWhen: "event:extract_data.completed && {{ extract_data.row_count > 0 }}"
  ↓
编排引擎解析表达式
  ↓
变量部分: {{ extract_data.row_count > 0 }}
  ↓
编排引擎 → 变量池.解析表达式("{{ extract_data.row_count > 0 }}")
  ↓
变量池:
  1. 获取 extract_data.row_count = 1000000
  2. 求值: 1000000 > 0 → true
  ↓
返回: true
```

**场景2：inputBindings 解析**

```
inputBindings: {
  "batch_size": "{{ extract_data.row_count / 1000 }}"
}
  ↓
编排引擎 → 变量池.解析表达式("{{ extract_data.row_count / 1000 }}")
  ↓
变量池:
  1. 获取 extract_data.row_count = 1000000
  2. 计算: 1000000 / 1000 = 1000
  ↓
返回: 1000
```

---

## 6. 表达式求值引擎

### 6.1 表达式语法

**基本语法**：使用 `{{ }}` 包裹表达式

**支持的操作**：

| 操作类型 | 语法 | 示例 |
|---------|------|------|
| 变量引用 | `{{ var_name }}` | `{{ extract_data.row_count }}` |
| 点号访问 | `{{ obj.field }}` | `{{ extract_data.output_df.row_count }}` |
| 算术运算 | `+`, `-`, `*`, `/`, `%` | `{{ a + b }}`, `{{ x * 100 }}` |
| 比较运算 | `>`, `<`, `>=`, `<=`, `==`, `!=` | `{{ count > 1000 }}` |
| 逻辑运算 | `&&`, `||`, `!` | `{{ a > 0 && b > 0 }}` |
| 函数调用 | `func(args)` | `{{ len(array) }}`, `{{ max(a, b) }}` |

### 6.2 安全性设计

**沙箱环境**：

- 禁用文件IO操作
- 禁用网络请求
- 禁用系统调用
- 只允许安全的内置函数

**允许的内置函数**：

- 数学函数：`abs`, `min`, `max`, `round`, `sum`
- 字符串函数：`len`, `str`, `upper`, `lower`
- 数组函数：`len`, `count`

**禁用的危险函数**：

- `eval`, `exec`, `compile`
- `open`, `read`, `write`
- `import`, `__import__`
- `getattr`, `setattr`, `delattr`

### 6.3 表达式求值流程

```
输入: "{{ extract_data.row_count + transform_data.row_count }}"
  ↓
1. 解析表达式
   提取变量引用: [extract_data.row_count, transform_data.row_count]
  ↓
2. 构建上下文
   extract_data.row_count → 1000000
   transform_data.row_count → 500000
  ↓
3. 求值
   1000000 + 500000 → 1500000
  ↓
4. 返回结果
   1500000
```

### 6.4 复杂表达式示例

**嵌套对象访问**：

```
表达式: {{ extract_data.output_df.schema.user_id }}
  ↓
变量值: {
  "output_df": {
    "schema": {
      "user_id": "string",
      "event_time": "timestamp"
    }
  }
}
  ↓
结果: "string"
```

**条件表达式**：

```
表达式: {{ quality_score >= 0.9 && row_count > 1000 }}
  ↓
变量值:
  quality_score = 0.95
  row_count = 5000
  ↓
求值:
  0.95 >= 0.9 → true
  5000 > 1000 → true
  true && true → true
  ↓
结果: true
```

---

## 7. 变量访问控制实现

### 7.1 访问控制检查点

**检查点1：变量解析时**

编排引擎在解析 `inputBindings` 或表达式时，变量池需要验证访问权限：

```
解析请求: 获取 "quality_check.score"
  ↓
变量池检查:
  1. 变量是否存在？
  2. 请求来源是否有权限访问？
  ↓
如果不存在或无权限 → 抛出错误
如果通过 → 返回变量值
```

**检查点2：节点执行前**

编排引擎在启动节点前，验证所有引用的变量都已就绪：

```
节点准备启动: transform_data
  ↓
解析 inputBindings 中的所有变量引用
  ↓
检查每个引用的变量:
  - extract_data.output_path ✅ 存在
  - extract_data.row_count ✅ 存在
  ↓
所有变量就绪 → 启动节点
有变量缺失 → 等待或报错
```

### 7.2 访问权限矩阵

| 访问源 | pipeline.input.* | system.* | 已完成节点输出 | 未来节点输出 | 其他执行实例 | 子流水线内部 |
|--------|-----------------|----------|--------------|------------|-------------|-------------|
| **当前节点** | ✅ 允许 | ✅ 允许 | ✅ 允许 | ❌ 禁止 | ❌ 禁止 | ❌ 禁止 |
| **子流水线** | ❌ 禁止 | ❌ 禁止 | ❌ 禁止 | ❌ 禁止 | ❌ 禁止 | ✅ 允许（自己的） |
| **表达式求值** | ✅ 允许 | ✅ 允许 | ✅ 允许 | ❌ 禁止 | ❌ 禁止 | ❌ 禁止 |

### 7.3 错误处理

**场景1：访问不存在的变量**

```
表达式: {{ undefined_node.output }}
  ↓
变量池查询: undefined_node.output
  ↓
结果: 变量不存在
  ↓
抛出错误:
  ErrorCode: VARIABLE_NOT_FOUND
  Message: "Variable 'undefined_node.output' not found in execution exec_12345"
  Suggestion: "Check if node 'undefined_node' has completed and produced output"
```

**场景2：访问不存在的变量**

```
节点A: inputBindings: { data: "{{ 节点B.output }}" }
节点A 触发时，节点B 尚未执行
  ↓
变量池查询: 节点B.output
  ↓
结果: 变量不存在
  ↓
抛出错误:
  ErrorCode: VARIABLE_NOT_FOUND
  Message: "Variable '节点B.output' not found in execution exec_12345"
  Suggestion: "Ensure node '节点B' has completed before referencing its outputs"
```

**场景3：子流水线访问父流水线变量**

```
子流水线节点: inputBindings: { data: "{{ parent_node.output }}" }
  ↓
变量池查询: parent_node.output（在子流水线执行空间）
  ↓
结果: 变量不存在（父流水线变量不在子流水线空间）
  ↓
抛出错误:
  ErrorCode: VARIABLE_NOT_ACCESSIBLE
  Message: "Cannot access parent pipeline variables from sub-pipeline"
  Suggestion: "Pass required data through sub-pipeline inputBindings"
```

### 7.4 访问控制的实现建议

**方法1：基于变量存在性（简单）**

- 优点：实现简单，自然遵循执行顺序
- 缺点：错误信息不够明确

```
流程:
  解析变量引用 → 查询变量池 → 不存在则报错
```

**方法2：基于静态分析（可选，辅助）**

- 优点：可以提前发现一些明显的错误
- 实现：在流水线启动前，分析变量引用

```
流程:
  1. 提取所有节点的 inputBindings 中的变量引用
  2. 检查引用的变量名是否符合命名规范
  3. 标记可能存在的问题（如拼写错误）
  4. 提供警告信息（不阻止执行）

注意：由于节点执行顺序由事件动态决定，静态分析无法完全验证正确性
```

**方法3：基于执行上下文（完整）**

- 优点：支持复杂的访问控制策略
- 实现：变量池维护访问控制上下文

```
访问上下文:
  execution_id: "exec_12345"
  current_node: "transform_data"
  completed_nodes: ["extract_data", "validate_input"]
  parent_execution_id: null  # 如果是子流水线，记录父流水线ID
  
变量访问时:
  1. 检查 execution_id 是否匹配
  2. 检查变量所属节点是否在 completed_nodes 中
  3. 检查是否访问了不允许的作用域
```

---

## 8. 性能优化

### 8.1 缓存机制

**表达式求值缓存**：

- 缓存已求值的表达式结果
- Key: (execution_id, expression)
- Value: 求值结果
- 失效条件：变量更新时清除相关缓存

**变量访问缓存**：

- 热点变量缓存在内存
- 定期刷新或基于事件失效

### 7.2 批量操作

**批量设置变量**：

```
批量设置变量(execution_id, scope, {
  "var1": value1,
  "var2": value2,
  "var3": value3
})
```

**批量获取变量**：

```
批量获取变量(execution_id, [
  "extract_data.row_count",
  "extract_data.output_path",
  "transform_data.quality_score"
])
```

**优势**：减少网络往返次数（分布式部署场景）

### 7.3 大对象优化

**策略**：

1. **元数据存储**：变量池只存路径和 schema
2. **延迟加载**：实际使用时才加载大对象
3. **引用传递**：执行引擎之间传递路径而非数据

**示例**：

```
# 节点A输出 DataFrame
extract_data 完成
  ↓
变量池存储: {
  "output_df": {
    "path": "s3://bucket/data/output.parquet",
    "schema": {...}
  }
}
  ↓
# 节点B使用 DataFrame
transform_data 启动
  ↓
获取变量: extract_data.output_df
  ↓
返回: {"path": "s3://...", "schema": ...}
  ↓
执行引擎根据路径加载数据
```

---

## 8. 监控和可观测性

### 8.1 关键指标

| 指标名称 | 说明 | 用途 |
|---------|------|------|
| `variables_count` | 变量数量（按执行实例） | 资源使用监控 |
| `variable_storage_size` | 变量存储大小 | 内存/存储监控 |
| `expression_eval_duration` | 表达式求值延迟 | 性能监控 |
| `variable_get_rate` | 变量访问QPS | 负载监控 |
| `expression_eval_errors` | 表达式求值错误次数 | 质量监控 |

### 8.2 日志记录

**变量设置日志**：

```
[INFO] Variable set
  execution_id: exec_12345
  variable: extract_data.row_count
  type: number
  value: 1000000
```

**表达式求值日志**（调试模式）：

```
[DEBUG] Expression evaluation
  execution_id: exec_12345
  expression: {{ extract_data.row_count > 1000 }}
  result: true
  duration: 3ms
```

**错误日志**：

```
[ERROR] Expression evaluation failed
  execution_id: exec_12345
  expression: {{ undefined_var.count }}
  error: Variable 'undefined_var' not found
```

### 8.3 调试接口

**导出所有变量**：

```
GET /api/v1/executions/{execution_id}/variables

Response:
{
  "execution_id": "exec_12345",
  "variable_count": 15,
  "variables": {
    "pipeline.input.start_date": {
      "value": "2025-01-01",
      "type": "string",
      "scope": "pipeline.input"
    },
    "extract_data.row_count": {
      "value": 1000000,
      "type": "number",
      "scope": "extract_data"
    }
  }
}
```

---

## 9. 设计约束和权衡

### 9.1 设计约束

| 约束 | 说明 | 影响 |
|------|------|------|
| **按执行隔离** | 每个执行实例独立的变量空间 | 避免跨执行污染 |
| **变量不可变** | 变量一旦设置不可修改 | 保证数据一致性 |
| **类型感知** | 变量有明确的类型 | 支持类型验证 |
| **大对象引用** | DataFrame 只存元数据 | 避免内存溢出 |

### 9.2 权衡决策

| 决策点 | 方案A | 方案B | 选择 | 理由 |
|--------|------|------|------|------|
| **存储方式** | 全内存 | 内存+对象存储 | **混合** | 小变量快速访问，大对象引用存储 |
| **变量修改** | 可修改 | 不可修改 | **不可修改** | 简化逻辑，保证一致性 |
| **类型推断** | 自动推断 | 强制声明 | **自动推断** | 减少配置负担 |
| **表达式引擎** | 模板引擎 | 完整编程语言 | **模板引擎** | 安全性优先 |

### 9.3 扩展性考虑

**未来可能的扩展**：

1. **变量版本管理**：支持变量的多个版本（用于回滚）
2. **变量加密**：敏感变量加密存储
3. **变量权限**：不同节点访问不同的变量子集
4. **变量转换**：自动类型转换和数据清洗

**设计兼容性**：当前设计的接口足够抽象，支持未来扩展。

---

## 10. 总结

### 10.1 核心定位

变量池是编排系统的**数据交换中心**：

- **不是**：永久性的数据存储
- **是**：执行期间的变量缓存和表达式求值服务

### 10.2 设计原则

1. **分层命名空间**：清晰的变量作用域管理
2. **类型感知**：支持多种数据类型和大对象处理
3. **表达式求值**：支持灵活的变量引用和计算
4. **安全性优先**：使用沙箱环境防止恶意表达式
5. **高性能**：内存缓存 + 可选的分布式存储
6. **可观测性**：丰富的调试和监控能力

### 10.3 与 Dify 的对比

| 维度 | Dify | 本系统 |
|------|------|--------|
| **应用场景** | LLM 应用编排 | 数据流水线编排 |
| **变量类型** | string/number/object/file | + dataframe（大对象） |
| **命名空间** | global/node/conversation | pipeline.input/node_id/system |
| **表达式** | Jinja2 模板 | 类似 Jinja2 + 安全沙箱 |
| **大对象** | 文件路径 | DataFrame 元数据（路径+schema） |

### 10.4 与编排引擎的关系

```
编排引擎（核心）
  │
  ├─ 使用事件管理器 → 查询事件状态
  ├─ 使用变量池 → 解析变量和表达式
  └─ 使用执行引擎 → 运行任务

变量池（工具）
  └─ 提供数据服务给编排引擎
```

变量池是编排引擎进行数据传递的基础设施，参考 Dify 的设计，提供了强大的变量管理和表达式求值能力。
