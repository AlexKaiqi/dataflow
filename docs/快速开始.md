# 快速开始

## 系统能力概览

Dataflow 是一个轻量级的数据流编排平台，提供以下核心能力：

### 1. 任务定义 (TaskDefinition)

可复用的任务模板，定义数据处理任务的能力、输入输出和执行配置。

**支持的任务类型**：
- `PYSPARK_OPERATOR` - PySpark 批处理
- `SQL_OPERATOR` - SQL 查询
- `RAY_OPERATOR` - Ray 分布式算子（支持 DataJuicer 等）
- `STREAMING_OPERATOR` - 流处理
- `APPROVAL` - 审批流程
- `MODEL_INFERENCE` - 模型推理

**示例**：定义一个数据清洗任务，输入原始数据路径，输出清洗后的数据和统计信息。

### 2. 流水线编排 (Pipeline)

将多个任务组合成完整的数据处理流程：

```yaml
# 流水线定义
namespace: "ai.pipelines"
name: "training_pipeline"
description: "模型训练数据准备流水线"

# 流水线的输入参数
inputs:
  - name: raw_data_path
    type: string
  - name: output_path
    type: string

# 节点列表：定义要执行的任务及其执行时机
nodes:
  # 节点 1：数据清洗
  - id: "clean"
    type: task                                    # 引用任务定义
    taskDefinition:
      ref: "ai.tasks:data_cleaner:1.0.0"          # 任务引用：namespace:name:version
    inputBindings:                                # 绑定输入变量
      input_path: "{{ pipeline.input.raw_data_path }}"
      output_path: "{{ pipeline.input.output_path }}/cleaned/"
    startWhen: "event:pipeline.started"           # 流水线启动时执行
  
  # 节点 2：特征提取（依赖数据清洗完成）
  - id: "extract"
    type: task
    taskDefinition:
      ref: "ai.tasks:feature_extractor:1.0.0"
    inputBindings:
      input_path: "{{ clean.output_path }}"       # 引用上一个节点的输出
    startWhen: "event:clean.completed"            # clean 完成时执行

# 流水线的输出
outputs:
  - name: final_path
    value: "{{ extract.output_path }}"
```

**关键概念**：
- **节点 (Node)**：流水线中的执行单元，通过 `taskDefinition.ref` 引用已定义的任务
- **输入绑定 (inputBindings)**：将任务的输入变量绑定到具体值（流水线输入、上游节点输出、常量）
- **执行时机 (startWhen)**：定义节点何时开始执行（订阅流水线启动事件、上游节点完成事件等）

### 3. 版本管理

- 支持任务定义的多版本管理
- 草稿版本 (DRAFT) 和已发布版本 (PUBLISHED)
- 流水线可以引用特定版本的任务

---

## Ray 算子支持

### 什么是 Ray 算子？

Ray 算子任务 (`RAY_OPERATOR`) 是一种支持声明式多算子组合的数据处理任务类型。它基于 Ray 分布式框架，允许您通过配置多个算子来构建数据处理流水线。

**支持的算子框架**：
- **DataJuicer** - 数据清洗和预处理
- **自定义算子** - 基于 Ray 的自定义算子
- **第三方算子** - 其他 Ray 生态算子

### 在 Pipeline 中使用 Ray 算子

```yaml
namespace: "ai.pipelines"
name: "text_processing"
description: "文本数据处理流水线"

nodes:
  # 节点 1：数据清洗（内联定义算子链）
  - id: "clean"
    type: task
    taskDefinition:
      inline:
        type: RAY_OPERATOR
        inputVariables:
          - name: input_path
            type: string
          - name: output_path
            type: string
        outputVariables:
          - name: output_path
            type: string
          - name: rows_output
            type: number
        executionConfig:
          framework: "datajuicer"
          operators:                             # 算子链：按顺序处理同一数据集
            - type: "filter"                     # 1. 过滤空值
              config:
                columns: ["text"]
            - type: "map"                        # 2. 文本规范化
              config:
                column: "text"
                lowercase: true
            - type: "dedup"                      # 3. 去重
              config:
                method: "fuzzy"
                threshold: 0.95
          resources:
            cpu: 4
            memory: "8G"
    inputBindings:
      input_path: "{{ pipeline.input.input_path }}"
      output_path: "/data/cleaned/"
    startWhen: "event:pipeline.started"
  
  # 节点 2：特征提取
  - id: "extract"
    type: task
    taskDefinition:
      ref: "ai.tasks:feature_extractor:1.0.0"    # 也可以引用预定义的任务
    inputBindings:
      input_path: "{{ clean.output_path }}"
    startWhen: "event:clean.completed"
```

**说明**：

- Ray 算子任务的 `operators` 形成一个**处理链**，对同一数据集进行链式处理
- 算子之间通过数据集自动流转，无需显式指定中间输出
- 任务的 `inputVariables`/`outputVariables` 是任务对外的接口

---

## 典型场景：LLM 训练数据准备

### 场景描述

为大语言模型训练准备文本数据：数据清洗（Ray 算子）→ 格式转换（PySpark）。

### 完整示例

```yaml
namespace: "ai.pipelines"
name: "data_preparation"

inputs:
  - name: raw_data_path
    type: string
  - name: output_base_path
    type: string

outputs:
  - name: final_path
    value: "{{ convert.output_path }}"
  - name: rows_output
    value: "{{ clean.rows_output }}"

nodes:
  # 节点 1：数据清洗（Ray 算子 - 算子链处理）
  - id: "clean"
    type: task
    taskDefinition:
      inline:
        type: RAY_OPERATOR
        inputVariables:
          - name: input_path
            type: string
          - name: output_path
            type: string
        outputVariables:
          - name: output_path
            type: string
          - name: rows_output
            type: number
        executionConfig:
          framework: "datajuicer"
          operators:                   # 多个算子形成处理链
            - type: "filter"           # 1. 过滤空值
              config:
                columns: ["text"]
            - type: "map"              # 2. 文本规范化
              config:
                column: "text"
                fix_unicode: true
                remove_extra_spaces: true
            - type: "filter"           # 3. 质量过滤
              config:
                min_length: 50
                max_length: 10000
            - type: "dedup"            # 4. 去重
              config:
                method: "fuzzy"
                threshold: 0.85
          resources:
            cpu: 8
            memory: "16G"
    inputBindings:
      input_path: "{{ pipeline.input.raw_data_path }}"
      output_path: "{{ pipeline.input.output_base_path }}/cleaned/"
    startWhen: "event:pipeline.started"
  
  # 节点 2：格式转换（PySpark - 传统大数据处理）
  - id: "convert"
    type: task
    taskDefinition:
      ref: "ai.tasks:format_converter:1.0.0"  # 引用预定义的 PySpark 任务
    inputBindings:
      input_path: "{{ clean.output_path }}"
      output_path: "{{ pipeline.input.output_base_path }}/final/"
    startWhen: "event:clean.completed"
```

### 执行流水线

```bash
curl -X POST http://localhost:8080/api/pipelines/execute \
  -H "Content-Type: application/json" \
  -d '{
    "namespace": "ai.pipelines",
    "name": "data_preparation",
    "version": "1.0.0",
    "inputs": {
      "raw_data_path": "hdfs://data/raw/corpus/"
    }
  }'
```

---

## 下一步

- 查看 [TaskTypes 文档](./领域模型设计/TaskTypes/) 了解更多任务类型
- 阅读 [Ray 算子详细文档](./领域模型设计/TaskTypes/RayOperator.md)
- 探索 [API 文档](./api/) 了解如何通过 API 管理任务和流水线
- 参考 [测试规范](./测试规范/) 编写单元测试和集成测试
