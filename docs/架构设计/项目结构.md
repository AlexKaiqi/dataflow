# 项目整体架构

## 1. 架构定位

**系统本质**：基于 YAML 定义的**流水线编排系统**（Pipeline Orchestration System）

**核心能力**：
- 通过声明式 YAML 定义数据处理流水线
- 使用事件表达式（`startWhen`、`stopWhen` 等）描述节点触发条件
- 管理节点间的数据传递（变量池）
- 调度和执行各类任务（PySpark、SQL、Approval 等）

**对比参考**：
- 类似 **Airflow** 的 DAG 编排，但用表达式替代直接定义边
- 类似 **Argo Workflows**，但更专注于数据处理场景
- 借鉴 **Dify** 的变量池设计

---

## 2. 整体分层架构

```
┌─────────────────────────────────────────────────────────┐
│                   API & UI Layer                        │
│  - REST API (FastAPI)                                   │
│  - YAML Parser & Validator                              │
│  - UI (React/Vue) → YAML 转换                           │
└─────────────────────────────────────────────────────────┘
                           ↓
┌─────────────────────────────────────────────────────────┐
│              Application Service Layer                  │
│  - PipelineService (创建、发布、启动)                      │
│  - ExecutionService (执行管理、状态查询)                   │
│  - TaskService (任务定义管理)                             │
└─────────────────────────────────────────────────────────┘
                           ↓
┌─────────────────────────────────────────────────────────┐
│                   Domain Layer                          │
│  聚合根:                                                 │
│   - PipelineDefinition (流水线定义)                      │
│   - TaskDefinition (任务定义)                            │
│   - PipelineExecution (流水线执行实例)                    │
│  领域服务:                                                │
│   - ExecutionOrchestrator (编排引擎) ← 核心              │
└─────────────────────────────────────────────────────────┘
                           ↓
┌─────────────────────────────────────────────────────────┐
│              Orchestration Components                   │
│                                                          │
│  ┌─────────────────┐  ┌──────────────────┐             │
│  │  Event Manager  │  │  Variable Pool   │             │
│  │  事件管理器      │  │  变量池           │             │
│  └─────────────────┘  └──────────────────┘             │
│        ↓                      ↓                         │
│  ┌──────────────────────────────────────┐              │
│  │     Execution Orchestrator           │              │
│  │     编排引擎（核心调度逻辑）            │              │
│  └──────────────────────────────────────┘              │
└─────────────────────────────────────────────────────────┘
                           ↓
┌─────────────────────────────────────────────────────────┐
│              Execution Engine Layer                     │
│  - PySparkExecutor (Spark 任务执行器)                    │
│  - SQLExecutor (SQL 任务执行器)                          │
│  - ApprovalExecutor (审批任务执行器)                      │
│  - SubPipelineExecutor (子流水线执行器)                   │
└─────────────────────────────────────────────────────────┘
                           ↓
┌─────────────────────────────────────────────────────────┐
│              Infrastructure Layer                       │
│  - Repository (持久化)                                   │
│  - Resource Manager (资源管理)                           │
│  - External System Adapter (外部系统集成)                │
│  - Monitoring & Logging (监控日志)                       │
└─────────────────────────────────────────────────────────┘
```

---

## 3. 核心组件详解

### 3.1 Execution Orchestrator（编排引擎）
**职责**：流水线执行的核心调度器

**核心功能**：
1. **初始化执行**：创建 PipelineExecution，初始化所有 NodeExecution
2. **触发判断**：判断哪些节点可以执行（通过查询事件管理器和变量池）
3. **节点调度**：解析 inputBindings，创建 TaskExecution 或子 PipelineExecution
4. **状态管理**：更新节点和流水线状态，判断完成条件
5. **异常处理**：处理重试、失败、取消逻辑

**工作流程**：
```
循环检测：
  while PipelineExecution 未完成:
    1. 查询所有 pending 状态的节点
    2. 对每个节点：
       - 评估 startWhen 表达式（查询事件管理器）
       - 如果满足：
         a. 从变量池解析 inputBindings
         b. 调用执行引擎执行任务
         c. 更新节点状态为 running
    3. 监听任务完成事件
    4. 任务完成后：
       - 发布事件到事件管理器
       - 更新变量池
       - 更新节点状态
    5. 检查流水线是否完成
```

### 3.2 Event Manager（事件管理器）
**职责**：管理执行过程中的事件发布和查询

**设计原则**：
- **不是消息队列**：只是事件的存储和查询服务
- **不负责触发**：不主动通知订阅者，由编排引擎主动查询
- **简单明确**：只管理当前执行实例的事件

**核心接口**：
```python
class EventManager:
    def publish_event(self, execution_id: str, event: Event) -> None:
        """发布事件（存储事件记录）"""
        
    def query_events(
        self, 
        execution_id: str, 
        event_pattern: str,
        since: Optional[datetime] = None
    ) -> List[Event]:
        """查询事件（用于表达式评估）"""
        
    def has_event(
        self, 
        execution_id: str, 
        event_pattern: str
    ) -> bool:
        """检查事件是否存在（快速判断）"""
        
    def get_event_payload(
        self, 
        execution_id: str, 
        event_name: str
    ) -> Optional[Dict]:
        """获取事件负载（用于条件判断）"""
```

**数据结构**：
```python
@dataclass
class Event:
    event_id: str                    # 事件ID
    execution_id: str                # 所属执行实例
    event_type: str                  # 事件类型：node_id.completed
    timestamp: datetime              # 时间戳
    source: str                      # 事件源：node_id 或 pipeline
    payload: Dict[str, Any]          # 事件负载（可选）
```

**存储方案**：
- **内存存储**（单实例）：使用 Dict[execution_id, List[Event]]
- **数据库存储**（分布式）：PostgreSQL 表，索引 (execution_id, event_type)
- **可选 Redis**：用于分布式部署的共享状态

### 3.3 Variable Pool（变量池）
**职责**：管理执行过程中的变量和数据传递

**设计参考 Dify**：
- **分层命名空间**：pipeline.input.*, node_id.*, system.*
- **类型感知**：支持 string、number、object、array、file（路径）
- **大对象处理**：DataFrame 等大对象只存储元数据（路径、schema）

**核心接口**：
```python
class VariablePool:
    def set_variable(
        self, 
        execution_id: str, 
        scope: str,          # pipeline.input | node_id | system
        name: str, 
        value: Any,
        type_hint: Optional[str] = None
    ) -> None:
        """设置变量"""
        
    def get_variable(
        self, 
        execution_id: str, 
        reference: str       # 如 "node_a.output_path"
    ) -> Any:
        """获取变量（支持点号访问）"""
        
    def resolve_expression(
        self, 
        execution_id: str, 
        expression: str      # 如 "{{ node_a.count + node_b.count }}"
    ) -> Any:
        """解析表达式"""
        
    def get_all_variables(
        self, 
        execution_id: str
    ) -> Dict[str, Any]:
        """获取所有变量（用于调试）"""
```

**数据结构**：
```python
@dataclass
class Variable:
    name: str                        # 变量名
    scope: str                       # 作用域：pipeline.input | node_id
    value: Any                       # 变量值
    type: str                        # 类型：string | number | object | array | file
    metadata: Optional[Dict]         # 元数据（如文件路径、schema）
    created_at: datetime
```

**存储分层**：
```
execution_12345:
  pipeline.input.start_date: "2025-01-01"
  pipeline.input.end_date: "2025-01-31"
  
  extract_data.output_path: "s3://bucket/data/output.parquet"
  extract_data.row_count: 1000000
  
  transform_data.output_path: "s3://bucket/data/transformed.parquet"
  transform_data.quality_score: 0.95
  
  system.execution_id: "execution_12345"
  system.started_at: "2025-01-15T10:00:00Z"
```

---

## 4. 执行流程示例

### Pipeline 启动到节点触发的完整流程

```python
# 1. 用户启动流水线
POST /api/v1/pipelines/feature_engineering/start
{
  "version": "1.0.0",
  "inputVariables": {
    "start_date": "2025-01-01",
    "end_date": "2025-01-31"
  }
}

# 2. ExecutionOrchestrator 初始化
orchestrator.start_execution(pipeline_def, input_vars)
  ├─ 创建 PipelineExecution (status=running)
  ├─ 初始化所有 NodeExecution (status=pending)
  ├─ 写入 pipeline.input.* 到 VariablePool
  └─ 发布 "pipeline.started" 事件到 EventManager

# 3. 编排引擎主循环
while not execution.is_finished():
    # 查询所有 pending 节点
    for node in execution.get_pending_nodes():
        
        # 评估 startWhen 表达式
        if evaluate_trigger(node.startWhen):
            
            # 从变量池解析输入
            inputs = variable_pool.resolve_bindings(
                execution_id, 
                node.inputBindings
            )
            
            # 调用执行引擎
            task_execution = executor.execute(
                task_def=node.taskDefinition,
                inputs=inputs
            )
            
            # 更新节点状态
            node.status = "running"
            node.execution_id = task_execution.id
    
    # 等待任务完成
    completed_tasks = wait_for_completions()
    
    # 处理完成的任务
    for task in completed_tasks:
        # 发布事件
        event_manager.publish_event(
            execution_id,
            Event(
                event_type=f"{node.id}.completed",
                payload=task.outputs
            )
        )
        
        # 更新变量池
        variable_pool.set_variables(
            execution_id,
            scope=node.id,
            variables=task.outputs
        )
        
        # 更新节点状态
        node.status = "completed"
        node.outputs = task.outputs
    
    # 检查流水线是否完成
    if all_nodes_finished():
        execution.status = "completed"
        event_manager.publish_event(
            execution_id,
            Event(event_type="pipeline.completed")
        )
```

### startWhen 表达式评估示例

```python
# 节点定义
node:
  id: transform_data
  startWhen: "event:extract_data.completed && {{ extract_data.row_count > 0 }}"

# 评估过程
def evaluate_trigger(execution_id: str, expression: str) -> bool:
    # 1. 解析事件部分
    event_part = "event:extract_data.completed"
    has_event = event_manager.has_event(
        execution_id, 
        "extract_data.completed"
    )
    
    # 2. 解析条件部分
    condition_part = "{{ extract_data.row_count > 0 }}"
    row_count = variable_pool.get_variable(
        execution_id, 
        "extract_data.row_count"
    )
    condition_met = (row_count > 0)
    
    # 3. 组合判断
    return has_event and condition_met
```

---

## 5. 技术选型建议

### 5.1 事件管理器实现
**方案 1：内存 + 数据库**（推荐单机部署）
- 使用 Python Dict 缓存当前执行的事件
- 异步写入 PostgreSQL 用于审计和查询

**方案 2：Redis + 数据库**（推荐分布式部署）
- Redis Hash 存储事件：`execution:{id}:events`
- PostgreSQL 持久化历史事件

### 5.2 变量池实现
**方案 1：内存 + 对象存储**（推荐）
- 小变量（<1MB）：存储在内存/Redis
- 大对象（DataFrame、文件）：存储路径到 S3/MinIO
- 参考 Dify 的分层存储设计

### 5.3 表达式引擎
- **Jinja2**：变量替换 `{{ node_id.variable }}`
- **simpleeval**：安全表达式求值（禁用危险函数）
- **自定义 DSL 解析器**：解析 `event:xxx` 语法

---

## 6. 与 Airflow/Prefect 的对比

| 维度 | Airflow | 本系统 |
|------|---------|--------|
| **定义方式** | Python Code (DAG) | YAML (Pipeline) |
| **依赖表达** | 直接边关系 (`>>`) | 事件表达式 (`startWhen`) |
| **数据传递** | XCom | Variable Pool |
| **任务类型** | Operator (内置) | TaskDefinition (可扩展) |
| **调度方式** | DAG 拓扑排序 | 事件触发 + 表达式评估 |
| **适用场景** | 通用工作流 | 数据处理流水线 |

---

## 7. 下一步工作

**优先级 P0**（核心实现）：
1. ✅ 设计 Execution Orchestrator 的详细算法
2. ✅ 实现 Event Manager 的核心接口
3. ✅ 实现 Variable Pool 的核心接口
4. ✅ 实现表达式求值引擎

**优先级 P1**（完善功能）：
5. ✅ 实现各类 Executor (PySpark, SQL, Approval)
6. ✅ 实现重试和错误处理逻辑
7. ✅ 实现子流水线执行
8. ✅ 实现资源管理和调度

**优先级 P2**（运维支持）：
9. ✅ 监控和可观测性
10. ✅ UI 和可视化
11. ✅ 分布式部署方案
