# 快速开始

## 系统能力概览

Dataflow 是一个轻量级的数据流编排平台，提供以下核心能力：

### 1. 任务定义 (TaskDefinition)

可复用的任务模板，定义数据处理任务的能力、输入输出和执行配置。

**支持的任务类型**：
- `PYSPARK_OPERATOR` - PySpark 批处理
- `SQL_OPERATOR` - SQL 查询
- `RAY_OPERATOR` - Ray 分布式算子（支持 DataJuicer 等）
- `STREAMING_OPERATOR` - 流处理
- `APPROVAL` - 审批流程
- `MODEL_INFERENCE` - 模型推理

**示例**：定义一个数据清洗任务，输入原始数据路径，输出清洗后的数据和统计信息。

### 2. 流水线编排 (Pipeline)

将多个任务组织成有向无环图 (DAG)，通过事件驱动实现任务间的依赖和编排。

**示例**：数据清洗 → 特征提取 → 模型训练，前一个任务完成后自动触发下一个任务。

### 3. 版本管理

- 支持任务定义的多版本管理
- 草稿版本 (DRAFT) 和已发布版本 (PUBLISHED)
- 流水线可以引用特定版本的任务

---

## Ray 算子支持

### 什么是 Ray 算子？

Ray 算子任务 (`RAY_OPERATOR`) 是一种支持声明式多算子组合的数据处理任务类型。它基于 Ray 分布式框架，允许您通过配置多个算子来构建数据处理流水线。

**支持的算子框架**：
- **DataJuicer** - 数据清洗和预处理
- **自定义算子** - 基于 Ray 的自定义算子
- **第三方算子** - 其他 Ray 生态算子

### 在 Pipeline 中使用 Ray 算子

#### 步骤 1: 定义 Ray 算子任务

```yaml
# tasks/data_cleaner.yaml
namespace: "ai.tasks"
name: "text_cleaner"
type: RAY_OPERATOR
description: "文本数据清洗"

inputVariables:
  - name: input_path
    type: string
    required: true
  - name: output_path
    type: string
    required: true

outputVariables:
  - name: rows_processed
    type: number
  - name: rows_filtered
    type: number

executionConfig:
  framework: "datajuicer"              # 使用 DataJuicer 框架
  operators:
    - type: "filter"                   # 过滤算子
      name: "remove_nulls"
      config:
        columns: ["text", "label"]
    
    - type: "map"                      # 映射算子
      name: "normalize_text"
      config:
        column: "text"
        lowercase: true
        remove_punctuation: true
    
    - type: "dedup"                    # 去重算子
      name: "deduplicate"
      config:
        columns: ["text"]
        method: "fuzzy"
        threshold: 0.95
  
  resources:
    cpu: 4
    memory: "8G"
```

#### 步骤 2: 在 Pipeline 中引用

```yaml
# pipelines/data_pipeline.yaml
namespace: "ai.pipelines"
name: "text_processing"
description: "文本数据处理流水线"

nodes:
  - id: "clean"
    type: task
    taskDefinition:
      ref: "ai.tasks:text_cleaner:1.0.0"  # 引用 Ray 算子任务
    inputBindings:
      input_path: "{{ pipeline.input.input_path }}"
      output_path: "/data/cleaned/"
    startWhen: "event:pipeline.started"
  
  - id: "tokenize"
    type: task
    taskDefinition:
      ref: "ai.tasks:text_tokenizer:1.0.0"
    inputBindings:
      input_path: "{{ clean.output_path }}"  # 依赖清洗任务的输出
    startWhen: "event:clean.completed"       # 订阅清洗任务完成事件
```

---

## 典型场景：AI 训练数据准备

### 场景描述

为大语言模型 (LLM) 训练准备高质量的文本数据，包括数据清洗、质量过滤、去重和格式转换。

### 完整示例

#### 1. 定义数据清洗任务

```yaml
# tasks/llm_data_cleaner.yaml
namespace: "ai.llm.tasks"
name: "data_cleaner"
type: RAY_OPERATOR
description: "LLM 训练数据清洗"

inputVariables:
  - name: input_path
    type: string
    required: true
    description: "原始数据路径 (JSONL 格式)"
  
  - name: output_path
    type: string
    required: true
    description: "清洗后数据路径"
  
  - name: min_quality_score
    type: number
    required: false
    default: 0.8
    description: "最低质量分数"

outputVariables:
  - name: rows_input
    type: number
    description: "输入行数"
  
  - name: rows_output
    type: number
    description: "输出行数"
  
  - name: filter_rate
    type: number
    description: "过滤率"

executionConfig:
  framework: "datajuicer"
  
  operators:
    # 1. 移除空值和异常数据
    - type: "filter"
      name: "remove_nulls"
      config:
        columns: ["text", "metadata"]
    
    # 2. 文本规范化
    - type: "map"
      name: "normalize_text"
      config:
        column: "text"
        lowercase: false              # 保留大小写（重要）
        remove_extra_spaces: true
        fix_unicode: true
    
    # 3. 质量过滤
    - type: "filter"
      name: "quality_filter"
      config:
        min_quality_score: "${min_quality_score}"
        min_length: 50
        max_length: 10000
        language: "zh"                # 中文过滤
    
    # 4. 模糊去重
    - type: "dedup"
      name: "fuzzy_dedup"
      config:
        columns: ["text"]
        method: "minhash"
        threshold: 0.85
        num_perm: 128
    
    # 5. 敏感信息过滤
    - type: "filter"
      name: "sensitive_filter"
      config:
        remove_pii: true              # 移除个人身份信息
        remove_toxic: true            # 移除有害内容
  
  resources:
    cpu: 16
    memory: "32G"
  
  batch_size: 10000
```

#### 2. 定义格式转换任务

```yaml
# tasks/format_converter.yaml
namespace: "ai.llm.tasks"
name: "format_converter"
type: RAY_OPERATOR
description: "数据格式转换为训练格式"

inputVariables:
  - name: input_path
    type: string
    required: true
  
  - name: output_path
    type: string
    required: true

outputVariables:
  - name: rows_converted
    type: number

executionConfig:
  framework: "datajuicer"
  
  operators:
    # 转换为 Parquet 格式
    - type: "transform"
      name: "to_parquet"
      config:
        source_format: "jsonl"
        target_format: "parquet"
        compression: "snappy"
    
    # 字段映射
    - type: "map"
      name: "field_mapping"
      config:
        mappings:
          text: "content"
          metadata.source: "source"
          metadata.timestamp: "created_at"
  
  resources:
    cpu: 8
    memory: "16G"
```

#### 3. 定义数据统计任务

```yaml
# tasks/data_stats.yaml
namespace: "ai.llm.tasks"
name: "data_statistics"
type: PYSPARK_OPERATOR
description: "计算数据统计信息"

inputVariables:
  - name: input_path
    type: string
    required: true

outputVariables:
  - name: total_rows
    type: number
  - name: avg_length
    type: number
  - name: unique_sources
    type: number

executionConfig:
  sparkConf:
    spark.executor.memory: "4G"
    spark.executor.cores: 2
  
  script: |
    df = spark.read.parquet(input_path)
    
    stats = {
        'total_rows': df.count(),
        'avg_length': df.selectExpr('avg(length(content))').first()[0],
        'unique_sources': df.select('source').distinct().count()
    }
    
    return stats
```

#### 4. 组装完整流水线

```yaml
# pipelines/llm_data_preparation.yaml
namespace: "ai.llm.pipelines"
name: "training_data_preparation"
description: "LLM 训练数据准备流水线"
version: "1.0.0"

inputs:
  - name: raw_data_path
    type: string
    description: "原始数据路径"
  
  - name: output_base_path
    type: string
    description: "输出基础路径"
  
  - name: quality_threshold
    type: number
    default: 0.8
    description: "质量阈值"

outputs:
  - name: final_data_path
    type: string
    description: "最终训练数据路径"
  
  - name: data_statistics
    type: object
    description: "数据统计信息"

nodes:
  # 节点 1: 数据清洗
  - id: "clean"
    type: task
    taskDefinition:
      ref: "ai.llm.tasks:data_cleaner:1.0.0"
    inputBindings:
      input_path: "{{ pipeline.input.raw_data_path }}"
      output_path: "{{ pipeline.input.output_base_path }}/cleaned/"
      min_quality_score: "{{ pipeline.input.quality_threshold }}"
    startWhen: "event:pipeline.started"
  
  # 节点 2: 格式转换
  - id: "convert"
    type: task
    taskDefinition:
      ref: "ai.llm.tasks:format_converter:1.0.0"
    inputBindings:
      input_path: "{{ clean.output_path }}"
      output_path: "{{ pipeline.input.output_base_path }}/final/"
    startWhen: "event:clean.completed"
  
  # 节点 3: 数据统计
  - id: "stats"
    type: task
    taskDefinition:
      ref: "ai.llm.tasks:data_statistics:1.0.0"
    inputBindings:
      input_path: "{{ convert.output_path }}"
    startWhen: "event:convert.completed"

# 流水线输出映射
outputMappings:
  final_data_path: "{{ convert.output_path }}"
  data_statistics:
    total_rows: "{{ stats.total_rows }}"
    avg_length: "{{ stats.avg_length }}"
    unique_sources: "{{ stats.unique_sources }}"
    filter_rate: "{{ clean.filter_rate }}"
```

### 执行流水线

```bash
# 通过 REST API 或 SDK 执行
curl -X POST http://localhost:8080/dataflow/api/pipelines/execute \
  -H "Content-Type: application/json" \
  -d '{
    "namespace": "ai.llm.pipelines",
    "name": "training_data_preparation",
    "version": "1.0.0",
    "inputs": {
      "raw_data_path": "hdfs://data/raw/corpus-2024/",
      "output_base_path": "hdfs://data/processed/llm-training/",
      "quality_threshold": 0.85
    }
  }'
```

### 预期结果

执行完成后，您将获得：

1. **清洗后的数据** - `/processed/llm-training/final/` 目录
2. **数据统计报告**：
   ```json
   {
     "total_rows": 5000000,
     "avg_length": 512,
     "unique_sources": 150,
     "filter_rate": 0.32
   }
   ```
3. **执行日志** - 每个任务的详细执行日志
4. **中间产物** - `/processed/llm-training/cleaned/` (可配置保留)

---

## 下一步

- 查看 [TaskTypes 文档](./领域模型设计/TaskTypes/) 了解更多任务类型
- 阅读 [Ray 算子详细文档](./领域模型设计/TaskTypes/RayOperator.md)
- 探索 [API 文档](./api/) 了解如何通过 API 管理任务和流水线
- 参考 [测试规范](./测试规范/) 编写单元测试和集成测试
